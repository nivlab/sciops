{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import read_csv, concat\n",
    "from tqdm import tqdm\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.33)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load reject data.\n",
    "reject = read_csv(os.path.join('data','reject.csv'))\n",
    "\n",
    "## Initialize metrics DataFrame.\n",
    "metrics = reject[['platform','subject','infreq']].copy()\n",
    "metrics['infreq'] = np.where(metrics['infreq'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Define Behavioral Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load behavioral data.\n",
    "data = read_csv(os.path.join('data','data.csv'))\n",
    "\n",
    "## Restrict participants.\n",
    "data = data.loc[data.subject.isin(reject.subject)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute metrics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Merge with metrics.\n",
    "metrics = metrics.merge(reject[['platform','subject','accuracy']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute metrics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Compute performance within participants.\n",
    "gb = data.groupby(['platform','subject']).outcome.sum().reset_index()\n",
    "gb = gb.rename(columns={'outcome':'points'})\n",
    "\n",
    "## Merge with metrics.\n",
    "metrics = metrics.merge(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Win Stay Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute metrics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Compute previous win.\n",
    "f = lambda x: np.roll(x, 1)\n",
    "data['prev_win'] = data.groupby(['platform','subject']).outcome.transform(f)\n",
    "data.loc[data.trial==1, 'prev_win'] = np.nan\n",
    "\n",
    "## Compute stay.\n",
    "f = lambda x: (x == np.roll(x,1)).astype(int)\n",
    "data['stay'] = data.groupby(['platform','subject']).choice.transform(f)\n",
    "data.loc[data.trial==1, 'stay'] = np.nan\n",
    "\n",
    "## Compute win-stay rate.\n",
    "gb = data.query('prev_win==1').groupby(['platform','subject']).stay.mean().reset_index()\n",
    "gb = gb.rename(columns={'stay':'ws'})\n",
    "\n",
    "## Merge with metrics.\n",
    "metrics = metrics.merge(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Lose Shift Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute metrics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Compute win-stay rate.\n",
    "gb = data.query('prev_win==0').groupby(['platform','subject']).stay.mean().reset_index()\n",
    "gb = gb.rename(columns={'stay':'ls'})\n",
    "gb['ls'] = 1 - gb['ls']\n",
    "\n",
    "## Merge with metrics.\n",
    "metrics = metrics.merge(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Perseveration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute metrics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define trial number within each block.\n",
    "data['exposure'] = data.groupby(['subject','block']).trial.transform(lambda x: np.arange(x.size)+1)\n",
    "\n",
    "## Define perseveration errors.\n",
    "data['perseveration'] = data.groupby('subject').correct.transform(lambda x: np.roll(x, 15))\n",
    "data['perseveration'] = (data['perseveration'] == data['choice']).astype(int)\n",
    "data.loc[data.block==1,'perseveration'] = np.nan\n",
    "\n",
    "## Compute perseveration errors within participants.\n",
    "gb = data.groupby(['platform','subject']).perseveration.mean().reset_index()\n",
    "\n",
    "## Merge with metrics.\n",
    "metrics = metrics.merge(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from pandas import DataFrame\n",
    "\n",
    "## Extract behavior.\n",
    "with h5py.File(os.path.join('stan_results',f'td_m1_np_mcmc.hdf5'), 'r') as hdf5:\n",
    "    theta_pr = hdf5['theta_pr'][:]\n",
    "    y_pred = hdf5['Y_pred'][:]\n",
    "    beta = hdf5['beta'][:]\n",
    "    eta_p = hdf5['eta_p'][:]\n",
    "    eta_n = hdf5['eta_n'][:]\n",
    "    \n",
    "## Convert to DataFrame.\n",
    "theta_pr = DataFrame(\n",
    "    np.column_stack([theta_pr.T, beta, eta_p, eta_n, y_pred]),\n",
    "    columns=['beta_pr','eta_pr','kappa_pr','beta','eta_p','eta_n','y_pred']\n",
    ")\n",
    "\n",
    "## Append metadata.\n",
    "theta_pr['platform'] = metrics.sort_values('subject').platform.values\n",
    "theta_pr['subject'] = metrics.sort_values('subject').subject.values\n",
    "\n",
    "## Merge with metrics.\n",
    "metrics = metrics.merge(theta_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Define Self-Report Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load survey data.\n",
    "surveys = read_csv(os.path.join('data','surveys.csv'))\n",
    "\n",
    "## Restrict participants.\n",
    "surveys = surveys.loc[surveys.subject.isin(metrics.subject)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Self-Report Sum Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define subscales.\n",
    "subscales = {\n",
    "    '7u':    ['7u7d-q01','7u7d-q03','7u7d-q04','7u7d-q06','7u7d-q07','7u7d-q08','7u7d-q13'],\n",
    "    '7d':    ['7u7d-q02','7u7d-q05','7u7d-q09','7u7d-q10','7u7d-q11','7u7d-q12','7u7d-q14'],\n",
    "    'gad7':  ['gad7-q01','gad7-q02','gad7-q03','gad7-q04','gad7-q05','gad7-q06','gad7-q07'],\n",
    "    'bis':   ['bisbas-q01','bisbas-q02','bisbas-q03','bisbas-q04'],\n",
    "    'bas':   ['bisbas-q05','bisbas-q06','bisbas-q07','bisbas-q08',\n",
    "              'bisbas-q09','bisbas-q10','bisbas-q11','bisbas-q12'],\n",
    "    'shaps': ['shaps-q01','shaps-q02','shaps-q03','shaps-q04','shaps-q05',\n",
    "              'shaps-q06','shaps-q07','shaps-q08','shaps-q09','shaps-q10',\n",
    "              'shaps-q11','shaps-q12','shaps-q13','shaps-q14'],\n",
    "}\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute metrics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Iteratively compute sum scores.\n",
    "for k, v in subscales.items():\n",
    "    \n",
    "    surveys[k] = surveys[v].sum(axis=1)\n",
    "    \n",
    "## Merge with metrics.\n",
    "metrics = metrics.merge(surveys[['subject','platform'] + list(subscales.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Primary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Percentile Bootstrap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define queries.\n",
    "queries = [\n",
    "    'infreq > -1',\n",
    "    'accuracy > 0.411',\n",
    "    'infreq == 0',\n",
    "    'accuracy > 0.411 and infreq == 0'\n",
    "]\n",
    "\n",
    "## Define variables of interest.\n",
    "rows = ['accuracy','points','ws','ls','perseveration','beta','eta_p','eta_n','kappa_pr']\n",
    "cols = ['7u','7d','gad7','bis','bas','shaps']\n",
    "\n",
    "## Define number of bootstraps.\n",
    "n_iter = 5000\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Percentile bootstrap. \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "np.random.seed(47404)\n",
    "\n",
    "## Preallocate space.\n",
    "corr = np.zeros(( len(queries), len(rows), len(cols) ))\n",
    "pval = np.zeros_like(corr)\n",
    "\n",
    "## Main loop.\n",
    "for i, query in enumerate(queries):\n",
    "    \n",
    "    ## Copy DataFrame.\n",
    "    df = metrics.query(query)[rows+cols].copy()\n",
    "    indices = np.arange(df.shape[0])\n",
    "    \n",
    "    ## Compute observed correlations.\n",
    "    corr[i] = df.corr(method='spearman').loc[rows,cols].values\n",
    "    \n",
    "    ## Preallocate space.\n",
    "    null = np.zeros((n_iter, len(rows), len(cols)))\n",
    "    \n",
    "    ## Iteratively compute null distribution.\n",
    "    for j in tqdm(range(n_iter)):\n",
    "        \n",
    "        ## Sample bootstrap sequence.\n",
    "        ix = np.random.choice(indices, indices.size, replace=True)\n",
    "\n",
    "        ## Compute bootstrap correlations.\n",
    "        null[j] = df.iloc[ix].corr(method='spearman').loc[rows,cols].values\n",
    "        \n",
    "    ## Compute p-values.\n",
    "    pval[i] = np.where(np.sign(corr[i]) > 0, null < 0, null > 0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    \n",
    "    sns.heatmap( np.where(pval[i] < 0.05, corr[i], 0), vmin=-0.3, vmax=0.3, center=0, cbar=False, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
