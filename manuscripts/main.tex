% set document class
\documentclass[a4paper,notitlepage,12pt]{article}

% load packages
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=0.8in]{geometry}
\usepackage{authblk}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{xr-hyper} % for cross-referencing between documents
\usepackage{hyperref}
\usepackage{apacite}
\usepackage{float}
\usepackage{amsmath}
\usepackage[flushmargin]{footmisc}
\usepackage[flushleft]{threeparttable}
\usepackage{array}
\usepackage[font=small, labelfont=it]{caption}
\usepackage{booktabs} % for pretty tables

% specify footnote margins
\renewcommand\footnotelayout{%
  \advance\leftskip 0.7cm
  \advance\rightskip 0.7cm
 } 

% specify bibliography and link style
\bibliographystyle{apacite}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}
\renewcommand\bibliographytypesize{\small}

% specific images path relative to the main .tex file 
\graphicspath{ {./figures/} }

% specify some custom commands
\renewcommand{\baselinestretch}{1.2}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand*{\Affilfont}{\normalsize}

% set up the author block
\setlength{\affilsep}{2em}   % set the space between author and affiliation
\author[1]{Samuel Zorowitz}
\author[1,2]{Daniel Bennett}
\author[1,3]{Yael Niv}
\affil[1]{Princeton Neuroscience Institute, Princeton University, USA}
\affil[2]{Department of Psychiatry, Monash University, Australia}
\affil[3]{Department of Psychology, Princeton University, USA}

% specify the title
\title{Inattentive responding can induce spurious associations between task behaviour and symptom measures}

% turn date off
\date{}

% Define paragraph formatting
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

% make abstract
\abstract{Abstract}

% page break before introduction
\clearpage

% Define paragraph formatting
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\section{Main}

Paragraph 1: Rise of online studies. Utility for clinical research. Need to define TASK somewhere here.

Paragraph 2: Drawbacks to online studies. Noise. Data quality overview. Focus on unsystematic noise.

Paragraph 3: Noise is not systematic. As has been pointed out, careless responding can overinflate prevalence of symptoms and can yield spurious correlations. OR Need to define TASK somewhere here.

Paragraph 4: Not clear the extent to which this is an issue. For example, if screening in both types of data, might be not an issue. Even without screening, behavioral screening might be enough. Finally, behavior is complex and might not show up.

Paragraph 5: our current results call into question.

\section{Results}

\subsection{Screening for C/IE responding is common for task behavior but not for self-report behavior}

The presence of C/IE responding in a dataset can result in spurious correlations between measures of behavior. As such, it is imperative for researchers to detect and remove those participants whom complete online experiments with insufficient effort. The extent to which experimenters evaluate the quality of both the task and self-report data they collect is unclear. We suspect that whereas screening of task data is standard practice, the screening of self-report data is far less prevalent in online human behavioral research. 

To provide quantitative support to this claim, we performed a cursory literature review of online human behavioral studies. (See Methods for details of the literature search and the list of studies included.) Briefly, we identified studies for inclusion through searches on Google Scholar using permutations of query terms related to online labor platforms, experimental paradigms, and symptom measures. We included studies that (a) recruited participants online, (b) measured behavior on at least one experimental task, and (c) measured behavior on at least one self-report symptom measure. This resulted in the inclusion of 49 studies. We then evaluated whether and how those studies performed task and self-report behavior screening. We note that this review was not meant to be systematic, but instead to provide a representative overview of the screening practices common to online behavioral studies.

Summary statistics about the prevalence and types of screening practices are provided in Table~\ref{tab:screening}. Of the 49 online behavioral studies meeting inclusion criteria, roughly 80\% (39/49) used at least one approach to identify C/IE responding in task behavior. Of those studies, just over half relied on a single evaluative method. As might be expected, there was considerable heterogeneity in task behavior screening methods. Most common was identifying participants whose performance was indistinguishable from chance-level on some measure of accuracy. Almost as common was screening based on response variability, or identifying participants who predominantly responded in the same fashion (e.g. used only 1 key). Less common approaches were screening based on response times and instruction comprehension questions. 

In contrast, only the minority of online behavioral studies in our sample used at least one approach to identify C/IE responding in self-report behavior. Of those studies the most common method was the use of attention checks, which operate under the assumption that certain responses are unlikely under inattentive responding. Attention checks can be subdivided into different categories such as instructed items, where participants are explicitly told which response to select, and infrequency items, where only one or a small set of responses are valid. Of those studies that specified what type of attention check was used, instructed items were the most common form of attention check. Only a handful of studies employed statistical or so-called unobtrusive screening methods such as outlier detection or personal consistency. 

In sum whereas screening for C/IE responding in task behavior is ostensibly the norm for online behavioral studies, screening self-report behavior is far less prevalent. One possible explanation for this pattern of results is that researchers (implicitly) assume that screening both task and self-report behavior is redundant. That is, researchers may believe that task and self-report screening methods will identify the same participants as having exhibited C/IE responding. To our knowledge, however, this crucial assumption has never been empirically verified. In the next section, we explicitly test this hypothesis in a large sample of online participants.


\begin{table}[t]
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{lrrrr}
\toprule
& \multicolumn{2}{c}{Task Screening} & \multicolumn{2}{c}{Self-Report Screening} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Frequency & \multicolumn{2}{c}{N=39 (80\%)} & \multicolumn{2}{c}{N=19 (39\%)} \\
\midrule
Method & Accuracy & 18 (46\%) & Attention Check & 17 (89\%) \\
& Variability & 15 (38\%) & Instructed & 10 (53\%) \\
& Response Time & 7 (18\%) & Unspecified & 5 (26\%) \\
& Comprehension & 5 (13\%) & Unobtrusive & 4 (21\%) \\
& Other & 16 (41\%) \\
\bottomrule
\end{tabular}
\caption{The prevalence and types of task and self-report behavior screening practices in a representative sample (N=49) of online behavioral studies.}
\label{tab:screening}
\end{table}


Before continuing, we wish to briefly highlight two additional results from our literature review. First, we found that the majority of studies used only one single method to detect C/IE responding. This is worrisome from the perspective of signal detection theory. As has been previously reported, measures of C/IE responding have different sensitivities: some measures will identify only the worst participants, whereas others will flag many more respondents. As such, using only a single measure runs the risk of false positives (in the case of high sensitivity) and false negatives (in the case of low sensitivity). Similarly, different measures are sensitive to different \emph{types} of C/IE responding. Using only one measure may thus only identify a subset of C/IE respondents. As has been discussed elsewhere, the use of multiple measures is encouraged to more accurately and sensitively identify C/IE responding in a dataset.

Second, we found that the majority of studies that did screen self-report data relied on instructed items. This is separately worrisome as a growing body of research disputes their efficacy for online studies. There is empirical evidence that these items are less sensitive online elsewhere. A quick search online reveals that instructed items are frequently discussed in forums used by workers in online labor markets. Most concerning, there is empirical evidence that of a subset of participants that screen for these items and respond randomly where else. We encourage other methods and, as above, a multitude of methods. 

\subsection{Task and self-report measures of C/IE responding}

To measure the correspondence between measures of C/IE responding based on task and self-report behaviors, we conducted an behavioral experiment in two prominent online labor marketplaces (see Methods for complete details). In total, 400 participants were recruited from Amazon Mechanical Turk (N=) and Prolific (N=) to complete an online study comprised of one behavioral task and several self-report symptom measures. We recruited participants from both platforms in order to improve the generalizability of our findings. The demographics of each sample are summarized in Table XX. 

Participants completed a 3-arm probabilistic reversal learning bandit task. During the task, participants were presented with three options and could only choose one, after which they received feedback (reward or no-reward). Participants were instructed to choose the option that yielded reward most often, which they would learn through trial-and-error. Feedback was probabilistic such that the best option on a given trial returned reward on 80\% of trials and the other options only 20\%. Importantly, the best option changed every 15 trials. Participants were made explicitly aware that the best option would change over time, but they were not told when or how often this would occur. Participants completed 90 trials of the task (5 total reversals). 
 
Prior to the start of the behavioral experiment, participants also completed four standard self-report symptom measures: the generalized anxiety disorder scale (GAD7), the 7-up/7-down, the BIS/BAS, and the Snaith-Hamilton Pleasure Scale. The measures were selected to provide a range of response distributions. For example, the GAD-7 and 7-up/7-down are substantially asymmetric.

To measure the level of correspondence between task- and self-report-based measures of C/IE responding, we computed a number of metrics from the task and self-report behavior of each subject. For the task behavior, we estimated the following metrics: response variability, accuracy, win-stay lose-shift, and fraction of trials with suspicious response times. For the self-report behavior, we estimated the following metrics: number of suspicious responses to infrequency items, inter-item standard deviation (ISD), personal reliability, Mahalanobis distance, and the per-item reading time. 

To provide an overall sense of the quality of the data, we first focus on the task accuracy and infrequency scores. Based on task accuracy, XX\% of participants were flagged as exhibiting C/IE responding. That is, XX\% of participants showed accuracy levels at levels indistinguishable from chance. In contrast, the infrequnecy items flagged XX\% of participants as exhibiting C/IE responding. This discrepancy between identification rates is already suggestive of differences between task and self-report screening measures. Interestingly, the rates of flagged participants differed by platform. 

Give overall rates here. Basically want to discuss: (1) different by platform, (2) different by method. 

Core result 1: continuous (spearman) correlaiton analysis. 

\subsection{}

\section{Discussion}

Topic XX: Continuum of human effort

Topic XX: not a problem specific to online studies, but probably worse there due to lack of monitoring and different incentives.

Topic XX: how many to expect across platforms and by method. mturk vs. prolific differences. cat and mouse problems.

Topic XX: incentives for performance in online tasks

Topic XX: Accidentally screening out participants of interest

\section{Methods}

\section{Supplement}

\bibliography{main}

\end{document}
