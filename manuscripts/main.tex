% set document class
\documentclass[a4paper,notitlepage,12pt]{article}

% load packages
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=0.8in]{geometry}
\usepackage{authblk}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{xr-hyper} % for cross-referencing between documents
\usepackage{hyperref}
\usepackage{apacite}
\usepackage{float}
\usepackage{amsmath}
\usepackage[flushmargin]{footmisc}
\usepackage[flushleft]{threeparttable}
\usepackage{array}
\usepackage[font=small, labelfont=it]{caption}
\usepackage{booktabs} % for pretty tables

% specify footnote margins
\renewcommand\footnotelayout{%
  \advance\leftskip 0.7cm
  \advance\rightskip 0.7cm
 } 

% specify bibliography and link style
\bibliographystyle{apacite}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}
\renewcommand\bibliographytypesize{\small}

% specific images path relative to the main .tex file 
\graphicspath{ {./figures/} }

% specify some custom commands
\renewcommand{\baselinestretch}{1.2}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand*{\Affilfont}{\normalsize}

% set up the author block
\setlength{\affilsep}{2em}   % set the space between author and affiliation
\author[1]{Samuel Zorowitz}
\author[1,2]{Daniel Bennett}
\author[1,3]{Yael Niv}
\affil[1]{Princeton Neuroscience Institute, Princeton University, USA}
\affil[2]{Department of Psychiatry, Monash University, Australia}
\affil[3]{Department of Psychology, Princeton University, USA}

% specify the title
\title{Inattentive responding can induce spurious associations between task behaviour and symptom measures}

% turn date off
\date{}

% Define paragraph formatting
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

% make abstract
\abstract{Abstract}

% page break before introduction
\clearpage

% Define paragraph formatting
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\section{Main}

Paragraph 1: Rise of online studies. Utility for clinical research. Need to define TASK somewhere here.

Paragraph 2: Drawbacks to online studies. Noise. Data quality overview. Focus on unsystematic noise.

Paragraph 3: Noise is not systematic. As has been pointed out, careless responding can overinflate prevalence of symptoms and can yield spurious correlations. OR Need to define TASK somewhere here.

Paragraph 4: Not clear the extent to which this is an issue. For example, if screening in both types of data, might be not an issue. Even without screening, behavioral screening might be enough. Finally, behavior is complex and might not show up.

Paragraph 5: our current results call into question.

In recent years, online labor markets (e.g. Amazon Mechanical Turk, Prolific, Crowdflower) have become increasingly popular as a source of research participants in the behavioral sciences \cite{stewart2017crowdsourcing}, largely due to the ease with which these services allow for collecting large, diverse samples. Similarly, the advantages of online data collection platforms have been recognized for clinical research \cite{chandler2016conducting} and are increasingly used in studies of computational psychiatry. Here they are offer additional advantages. The ability to collect large samples facilitates transdiagnostic analysis, studying symptom co-occurrence and relation to behaviors \cite{gillan2016taking, rutledge2019machine}. Online platforms makes it easier to recruit and study otherwise difficult-to-reach populations \cite{strickland2019use}. Online platforms also make re-recruitment easier, making it possible to validate the reliability of measurements tools \cite{enkavi2019large} and possibly study longitudinal processes over long timescales \cite{kothe2019retention}. Some examples here: gillan


Paragraph 2: These advantages not without costs: online means giving up the control of the laboratory. Participants may be distracted, multi-tasking, and with different incentives than usual in-lab participants. Fortunately, many previous studies have found data quality on online crowdsourcing websites not altogether different or worse than in-lab samples. This isn't to say bad data doesn't exist: estimates vary, but non-negligible data bad and new issues arise. For the most part, however, noise has been treated as unsystematic something that can be overcome with increased samples  \cite{gillan2016taking, chandler2020participant}. 

> Although typical undergraduate subject populations are often motivated to participate in studies because of an interest in psychology, MTurk participants are unsupervised and anonymous, complete surveys in unknown locations, and are motivated by financial incentives

We specifically want to draw attention to a less appreciated fact: when traits are rare, such as in psychiatry, careless responding is more likely to manifest as symptom endorsement \cite{chandler2020participant, ophir2020turker}. Rather than mask true correlations, this can induce spurious correlations in data. This effect is well documented in personality psychology, where the presence of careless respondents can induce correlations between questionnaires and bias estimated factors in factor analysis. To our knowledge, however, discussion of this point has been limited to associations between self-report measures and not with respect to behavioral experiments. There is no reason to expect, however, that this result should be limited to those domains and may similarly be an issue for computational psychiatry.

Paragraph 4: not enough to speculate -- must demonstrate this is an issue. what we gonna show: (a) the minority of individuals screen. (b) behavior and survey screens may not correlate. (c) these correlations do arise. We close with some recommendations based on simulations. 

\section{Results}

\subsection{Screening for C/IE responding is common for task behavior but not for self-report behavior}

The presence of C/IE responding in a dataset can result in spurious correlations between measures of behavior. As such, it is imperative for researchers to detect and remove those participants whom complete online experiments with insufficient effort. The extent to which experimenters evaluate the quality of both the task and self-report data they collect is unclear. We suspect that whereas screening of task data is standard practice, the screening of self-report data is far less prevalent in online human behavioral research. 

To provide quantitative support to this claim, we performed a cursory literature review of online human behavioral studies. (See Methods for details of the literature search and the list of studies included.) Briefly, we identified studies for inclusion through searches on Google Scholar using permutations of query terms related to online labor platforms, experimental paradigms, and symptom measures. We included studies that (a) recruited participants online, (b) measured behavior on at least one experimental task, and (c) measured behavior on at least one self-report symptom measure. This resulted in the inclusion of 49 studies. We then evaluated whether and how those studies performed task and self-report behavior screening. We note that this review was not meant to be systematic, but instead to provide a representative overview of the screening practices common to online behavioral studies.

Summary statistics about the prevalence and types of screening practices are provided in Table~\ref{tab:screening}. Of the 49 online behavioral studies meeting inclusion criteria, roughly 80\% (39/49) used at least one approach to identify C/IE responding in task behavior. Of those studies, just over half relied on a single evaluative method. As might be expected, there was considerable heterogeneity in task behavior screening methods. Most common was identifying participants whose performance was indistinguishable from chance-level on some measure of accuracy. Almost as common was screening based on response variability, or identifying participants who predominantly responded in the same fashion (e.g. used only 1 key). Less common approaches were screening based on response times and instruction comprehension questions. 

In contrast, only the minority of online behavioral studies in our sample used at least one approach to identify C/IE responding in self-report behavior. Of those studies the most common method was the use of attention checks, which operate under the assumption that certain responses are unlikely under inattentive responding. Attention checks can be subdivided into different categories such as instructed items, where participants are explicitly told which response to select, and infrequency items, where only one or a small set of responses are valid. Of those studies that specified what type of attention check was used, instructed items were the most common form of attention check. Only a handful of studies employed statistical or so-called unobtrusive screening methods such as outlier detection or personal consistency. 

In sum whereas screening for C/IE responding in task behavior is ostensibly the norm for online behavioral studies, screening self-report behavior is far less prevalent. One possible explanation for this pattern of results is that researchers (implicitly) assume that screening both task and self-report behavior is redundant. That is, researchers may believe that task and self-report screening methods will identify the same participants as having exhibited C/IE responding. To our knowledge, however, this crucial assumption has never been empirically verified. In the next section, we explicitly test this hypothesis in a large sample of online participants.

\begin{table}[t]
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{lrrrr}
\toprule
& \multicolumn{2}{c}{Task Screening} & \multicolumn{2}{c}{Self-Report Screening} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Frequency & \multicolumn{2}{c}{N=39 (80\%)} & \multicolumn{2}{c}{N=19 (39\%)} \\
\midrule
Method & Accuracy & 18 (46\%) & Attention Check & 17 (89\%) \\
& Variability & 15 (38\%) & Instructed & 10 (53\%) \\
& Response Time & 7 (18\%) & Unspecified & 5 (26\%) \\
& Comprehension & 5 (13\%) & Unobtrusive & 4 (21\%) \\
& Other & 16 (41\%) \\
\bottomrule
\end{tabular}
\caption{The prevalence and types of task and self-report behavior screening practices in a representative sample (N=49) of online behavioral studies.}
\label{tab:screening}
\end{table}

Before continuing, we wish to briefly highlight two additional results from our literature review. First, we found that the majority of studies used only one single method to detect C/IE responding. This is worrisome from the perspective of signal detection theory. As has been previously reported, measures of C/IE responding have different sensitivities: some measures will identify only the worst participants, whereas others will flag many more respondents. As such, using only a single measure runs the risk of false positives (in the case of high sensitivity) and false negatives (in the case of low sensitivity). Similarly, different measures are sensitive to different \emph{types} of C/IE responding. Using only one measure may thus only identify a subset of C/IE respondents. As has been discussed elsewhere, the use of multiple measures is encouraged to more accurately and sensitively identify C/IE responding in a dataset.

Second, we found that the majority of studies that did screen self-report data relied on instructed items. This is separately worrisome as a growing body of research disputes their efficacy for online studies. There is empirical evidence that these items are less sensitive online elsewhere. A quick search online reveals that instructed items are frequently discussed in forums used by workers in online labor markets. Most concerning, there is empirical evidence that of a subset of participants that screen for these items and respond randomly where else. We encourage other methods and, as above, a multitude of methods. 

\subsection{Task and self-report measures of C/IE responding}

In order to the measure the correspondence of screening metrics estimated from task and self-report behavior, we conducted an online behavioral experiment. A finale sample of 386 participants from the Amazon Mechanical Turk (N=186) and Prolific (N=200) online labor markets completed the study and were included for analysis (see Methods for details). As part of the experiment, participants completed a probabilistic reversal learning task and 5 self-report symptom measures. The reversal learning task required participants to learn through trial-and-error which of three choice options yielded reward most often. The self-report measures assayed psychiatric symptoms related to depression, mania, anxiety, reward motivation, and anhedonia. The self-report measures were intentionally chosen based on previous literature to have a variety of response distributions. 

We then estimated measures of C/IE responding from each participant's task and self-report behavior (see Methods for full description). In line with the narrative review, we screened participants' task behavior for C/IE responding using the following metrics: response variability, or the fraction of responses using only one key; accuracy, or the fraction of choices of the dominant response option; win-stay lose-shift rates, or the difference in likelihood of repeating the previous choice following a rewarded vs. unrewarded trial; and fraction of suspicious response times. In line with previous studies, we screened participants' self-report behavior for C/IE responding using the following metrics: infrequency items, or the number of suspicious responses to bogus items; inter-item standard deviation, or the dispersion of responses around a participant's modal response on a given subscale; personal reliability, or the correlation between multiple random split-halves of a participant's responses; Mahalonbis distance, or the extent to which a participant is an outlier with respect to the total sample of participants; and reading time, or the average time taken responding to each item. 

To provide an overall sense of the quality of the data, we first focus on the task accuracy and the infrequency item metrics as these represent the most common screening methods of each behavioral modality. Only 26 participants (7\%) were flagged as exhibiting choice behavior at or above chance levels. In contrast, 85 participants (22\%) responded suspiciously to one or more of the infrequency items while completing of self-report symptom inventories. This discrepancy in the proportion of participants flagged by each method is consistent with previous studies which have found differing sensitivities to C/IE responding. Though we will not address platform differences here, we note the proportions of flagged participants were marginally but significantly greater on Mechanical Turk compared to Prolific for both accuracy ($z=2.224, p=0.026$) and infrequency ($z = 2.223, p = 0.026$) metrics.

As a first test of correspondence between the behavior and self-report metrics, we computed pairwise correlations on the unthresholded (continuous) metric data using Spearman's rank correlation. We tested for significant correlations using a permutation test with 5000 permutations of the data, correcting for multiple comparisons using the family-wise error rate. The resulting pairwise correlation matrix is presented in Figure 1a. With the exception of reading time, the self-report metrics exhibited moderate to strong intra-item correlations. The strong correlations between ISD, personal reliability, and Mahalonobis D are unsurprising as these have been previously shown to be sensitive to statistically random responding. Their correlations with the infrequency items provides face validity for the latter. The lack of correlation with reading time is consistent with previous research and evidence that workers wait to submit pages to avoid suspicion. 

The pattern of correlations for the task-based metrics is noticeably different. The only significant correlation surviving multiple corrections is between accuracy and win-stay lose-shift. This makes intuitive sense as, by definition, a sensitivity to rewarding and unrewarding outcomes should guide a participant to choosing the dominant response option. Crucially, there were few surviving correlations between task- and self-report-based metrics. Only accuracy showed any significant correspondence between self-report based measures (specifically the infrequency items, personality reliability, and the Mahalonobis distance). Even still, the degree of correspondence between these metrics was roughly half that observed for the correlations between self-report metrics. This is initial evidence that task- and self-report-based measures of C/IE responding are not redundant. 

A second way to measure the correspondence of the screening metrics is to compare the overlap in the sets of participants each metric flags after thresholding. Though measures like accuracy and infrequency have relatively clear threshold cutoffs (e.g. chance level performance), the majority of the metrics evaluated here do not. As such, we evaluated the metrics with respect to the top 25\% and 10\% of suspicious participants flagged by each measure. Conveniently, these percentiles roughly correspond to the fraction of participants flagged by the infrequency items and accuracy metrics, respectively. For each percentile threshold, we computed pairwise similarities on the thresholded metric data using Dice similarity. As above, we tested for significant correlations using a permutation test with 5000 permutations of the data, correcting for multiple comparisons using the family-wise error rate.

The pairwise Dice similarity matrix at the 25\% and 10\% percentiles are presented in Figure 1b and 1c, respectively. At both threshold levels, the pattern of results resembles what was observed for the unthresholded correlations. With the exception of reading time again, the self-report metrics show moderate to strong levels of agreement when flagging the most suspicious participants. Conversely, the task-based metrics show little agreement with exception of the accuracy and win-stay lose-shift metrics. Again we find that on average the self-report and task metrics exhibit chance levels of agreement, with the exception of accuracy. As before, where there are significant correspondence between self-report and task metrics, they are roughly half that observed for correspondence among the self-report metrics. 

In sum, the correlation analyses suggest that demonstrate that in principle task- and self-report-based measures of C/IE responding should not be assumed to be redundant. If anything, the current data suggests that metrics derived from these two types of behavioral data show low overall levels of correspondence, at least in comparison to within-modality levels of correspondence. It is worth emphasizing that, in the threshold-based analyses, the highest level of correspondence never exceeded 0.4. That is, these two types of metrics are flagging different participants in the majority of cases. Though there may be instances where these metrics are in greater agreement (a point we will come back to in the discussion), an experimenter simply cannot assume that task-based screening is sufficient to identify all C/IE responding in self-report data.

Before turning to the consequences of failing to screen on self-report data, we first want to demonstrate that the self-report metrics are indeed sensitive to C/IE responding. Though much previous work using these metrics have already done so, it is worth verifying this is the case in the present data. Since we are measuring the relationship between task and self-report for the first time, it behooves us to make sure that these metrics are indeed sensitive.

The first approach is to compare the proportion of participants meeting clinical cutoffs before and after screening. In the following, we will use only infrequency items to screen out C/IE responding. Specifically, we will use a conservative threshold and cite as evidence any suspicious response to an infrequency item as evidence of C/IE responding. The fraction of participants at or above clinical levels of symptom endorsement for each symptom measure is reported in Table XX. Using a clinical cutoff score of 12, we observe substantial reductions in the fraction of participants endorsing moderate-to-high levels of symptoms on the the mania (7u) and depression (7d) subscales of the Seven-up/Seven-down symptom measure. Importantly, this screening reduces the overall rate of clinical endorsement in the remaining sample to levels observed in epidemiological studies of mania and depression. Similarly, using a clinical cutoff score of 10, we observe a similarly substantial reduction in the fraction of participants endorsing high levels of anxiety symptoms on the GAD-7. Interestingly, a substantial proportion of participants still endorse high levels of anxiety though this may reflect the time period in which this data was collected (during COVID-19 pandemic). Finally, SHAPS...

A second approach is to compare the internal consistency of the measures in the flagged vs. non-flagged samples. Because these symptom questionnaires are vetted in clinical populations, if the flagged participants were simply psychiatric participants with noisier responding we should still expect the internal consistency to be valid. We compute the internal consistency of each measure as the mean pairwise Spearman correlation of items within each subscale. The correlation structure is presented in Figure XX. As can be visually observed, the internal consistency (on-diagonal blocks) are on noticeably reduced between the two groups. We compare this difference statistically of the average of these scores using permutation testing by permuting the labels of the participants (flagged vs. non-flagged). We do this to control for differences in sample size between the two groups. We found a significant difference, indicating that this reduction is credible above and above differences in group size. This is additional that suggests that the flagged participants are truly engaging in C/IE responding. 

\begin{table}[t]
\centering
\setlength{\tabcolsep}{6pt}
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{2}{c}{Avg. Total Score} & \multicolumn{2}{c}{Fraction} & \multicolumn{2}{c}{Cronbach's $\alpha$} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 Subscale & No Fail & Any Fail & No Fail & Any Fail & No Fail & Any Fail \\
\midrule
7-up    &  3.87 &  10.15 &  0.04 &  0.45 &  0.84 &  0.84 \\
7-down   &  4.75 &  10.68 &  0.09 &  0.46 &  0.94 &  0.88 \\
GAD7  &  4.86 &   9.73 &  0.17 &  0.56 &  0.92 &  0.87 \\
PSWQ  &  4.78 &   6.74 &  0.07 &   0.08 &  0.93 &  0.81 \\
BIS   &  7.74 &   7.92 &   - &   - &  0.83 &  0.62 \\
BAS-R &  6.38 &   7.38 &   - &   - &  0.71 &  0.60 \\
BAS-D &  9.35 &   8.79 &   - &   - &  0.86 &  0.69 \\
SHAPS &  8.02 &  10.85 &   0.15 &   0.29 &  0.90 &  0.81 \\
\bottomrule
\end{tabular}
\caption{}
\label{tab:screening}
\end{table}

\subsection{Spurious correlations}

Paragraph 1: Distributions of responses between groups

Paragraph 2: behavioral metrics

Paragraph 3: Correlation structure 

Paragraph 4: Correlation structure by sample size

Paragraph 5: Model-agnostic analysis 

\section{Discussion}

Paragraph 1:N: restate results

Paragraph N+1: low correspondence between different measures highlights the need for utilizing multiple measures. this study highlights that metrics may be differentially sensitive, but also sensitive to different types of motivation. accuracy was insufficient on its own. This is worrisome as this as that is the modal screening form. However, the results also suggest that infrequency items were not perfectly sensitive on their own, as evidenced by mania. given that our literature review highlighted that many folks using only one method, this is concerning. 

Paragraph N+2: Relatedly, these results suggest that low-effort responding is not straightforward. Participants may feel unmotivated in some parts, but not others. This no doubt reflects, in part, the difficulty of the objective. Our RL task was not particularly difficult, so low effort could still do reasonable well. Separately, our experiments are setup to incentivize performance only on task but not self-report. 

Paragraph N+3: Briefly worth addressing platform differences. Should say differences are not surprising but we don't think that should be cause for alarm. Cite Cloudresearch tools. Cite better checks. 

Paragraph N+4: Limitations

Paragraph N+5: Future directions 1: how big of a problem is this. how many bad subjects do you need?

Paragraph N+6: Future directions 2: answers to above is in part limited by lack of better models of low-effort responding. more effort to study what heuristic responses would look like. And are these rational? 

Paragraph N+7: Future directions 3: really making sure we're not throwing baby out with bathwater. confirm actually low-effort vs. psychiatric

Conclusion: 

\section{Methods}

\subsection{Literature Review}

To quantify the prevalence and types of data screening practices, we conducted a literature review of online behavioral studies.

\subsection{Experiment}

\subsubsection{Sample}

409 total participants were recruited to participate in an online behavioral experiment in May, 2020. Specifically, 208 participants were recruited from Amazon Mechanical Turk (MTurk) and 201 participants were recruited from Prolific.  This study was approved by the Institutional Review Board of Princeton University (\#5291), and all participants provided written informed consent. Total study duration was approximately 10 minutes per participant. Participants received monetary compensation for their time (rate \$12/hr), plus an incentive-compatible bonus based on task performance. 

Participants from Mechanical Turk were recruited with the aid of CloudResearch services and were eligible if they resided in the United States or Canada. Following recent recommendations, Mechanical Turk workers were not excluded based on HIT approval rate or number of HITs approved. Participants from Prolific were similarly eligible if they resided in the United States. No other exclusion criteria were applied during recruitment. 

The data from multiple participants who completed the experiment were excluded prior to analysis. Three participants (all Mechanical Turk) were excluded due to missing data. In addition, 20 participants were excluded for having previously completed the experiment on the other platform (MTurk: N=19, Prolific: N=1). This left a final sample of N=386 participants (MTurk: N=186, Prolific: N=200) for analysis. The demographics of the sample split by labor market is provided in Table S1. Notably, the participants recruited from MTurk were older ($M = 7.7 \ \text{yrs}, t = 6.567, p < 0.001$) and comprised of fewer women ($z = 6.567, p = 0.011$). Interestingly, 38\% of MTurk users in our sample reported also completing work on Prolific, whereas only 14\% of Prolific users in our sample reported completing work on MTurk. 

\subsubsection{Experimental Task}

Participants performed a probabilistic reversal learning task, explicitly designed to be similar to previous computational psychiatry studies. On every trial of the task, participants were presented with three choice options and were required to choose one. After their choice, participants were presented with probabilistic feedback: a reward (1 point) or a non-reward (0 points). On any trial one choice option dominated the others. When chosen, the dominant option yielded reward with 80\% probability; the subordinate options yielded reward with only 20\% probability. Importantly, dominant option changed every 15 trials. Participants completed 90 trials of the task (1 learning block, 5 reversal blocks). 

As a cover story, the probabilistic reversal learning task was introduced to participants as a fishing game. In the game, each choice option was a beach scene made distinguishable by a colored surfboard with unique symbol. Participants were told they were choosing which beach to fish at. Feedback was presented as either a fish (1 point) or trash (0 points). Participants were instructed to earn the most points possible by learning (through trial-and-error) and choosing the best choice option. Participants were also instructed that the best option could change during the task, but were not informed about how often or when this would occur (see the Supplement for the complete instructions). Prior to beginning the experiment, participants had to correctly answer four comprehension questions about the instructions. Failing to correctly answer all items forced the participant to start the instructions over.

The task was programmed in jsPsych and distributed using custom web-application software. Both the experiment code and web-software is publicly available.

\subsubsection{Self-Report Measures}

Prior to completing the reversal learning task, participants completed 5 self-report symptom measures. The symptom measures were selected for inclusion based on their frequency in clinical research and for having an assorted range of response distributions (e.g. symmetric, right-skewed, left-skewed).

\textbf{Seven-Up/Seven-Down}: The Seven-Up/Seven-Down (7u/7d) scale is a 14-item measure of manic and depressive tendencies. It is an abbreviation of the General Behavior Inventory, wherein only items that maximally discriminated between depression and mania were included. It has good psychometric properties and predictive validity. Items are scored on 4-point scale from 0 (Never or hardly ever) to 3 (Very often; almost constantly). Total scores on both subscales are usually strongly right-skewed, with few participants exhibiting moderate to high levels of symptom endorsement. 

\textbf{Generalized Anxiety Disorder-7}: the Generalized Anxiety Disorder-7 (GAD-7) is a 7-item measure of general anxiety. It has good psychometric properties in primary care settings and in the general population. The GAD-7 assesses how much a respondent has been bothered by each of seven core anxiety symptoms over the last 2 weeks. Items are scored on 4-point scale from 0 (not at all) to 3 (nearly every day). Total scores on the GAD-7 are usually right-skewed, with few participants exhibiting moderate to high levels of symptom endorsement.

\textbf{Behavioral Inhibition/Behavioral Activation Scales}: the Behavioral Inhibition and Behavioral Activation Scales (BIS/BAS) are a measure of approach and avoidance motivations. The original 42-item measure was recently abbreviated to a 14-item measure, which we use here.

\textbf{Snaith-Hamilton Pleasure Scale}:

\textbf{Penn State Worry Questionnaire}:

\subsection{Analysis}

\subsubsection{Correspondence of screening measures}

To measure the correspondence of task- and self-report-based screening metrics, we estimated a number of standard measures of data quality from each participant's task and self-report behavior. Beginning first with the self-report data, we included five common measures of data quality, each described below.

\textbf{Infrequency items}: We embedded four infrequency items across the self-report measures. The infrequency items were questions that should have only one response. We measured the number of suspicious responses made by each participant. For thresholded analyses, participants were flagged if they responded incorrectly to any of these items.

\textbf{Inter-item standard deviation}: The inter-item standard deviation (ISD) is an intrapersonal measure of response variability. For a given subscale $j$, the ISD is defined as:

\begin{equation*}
    ISD_j = \sqrt{\frac{\sum^k_{j=1}(x_j - \bar{x_i})^2}{k-1}}
\end{equation*}

where $x_j$ is a participant's response to item $j$, $x_i$ is a respondent's average score across all scale items, and $k$ is the total number of scale items. The ISD measures the level of dispersion around a participant's average response, and has previously been shown to be sensitive to C/IE responding. A composite ISD was computed per participant by summing across each individual subscale.

\textbf{Personal reliability}: The personality reliability coefficient was computed by correlating the average score of random split-halves of items for each subscale. 1000 random split-halves were computed per subscale and participant.  A composite score was computed per participant by summing across each individual subscale. Lower scores indicate greater response inconsistency.

\textbf{Mahalanobis D}: The Mahalanobis distance is an outlier detection method. For a participant $i$, the Mahalanobis D is defined as:

\begin{equation*}
    D = \sqrt{(X_i - \bar{X})^T \cdot \Sigma^{-1}_{XX} \cdot (X_i - \bar{X})^T }
\end{equation*}

where $(X_i - \bar{X})$ represents the vector of mean-centered item responses for participant $i$ and $\Sigma^{-1}_{XX}$ represents the inverted covariance matrix of all items. s. Larger deviation from the normative response pattern yields higher D values and is considered a potential indicator of C/IE responding. A single $D$ statistic was computed for each of the participants using all self-report items.

\textbf{Response Time}: One checks were based on time measurements, with a reading-speed threshold of 10 words per second (Sisso, 2019).

In addition, four measures of data quality were computed from the task behavior data. These are described in turn below.

\textbf{Variability}: 

\textbf{Accuracy}: 

\textbf{Win-Stay Lose-Shift}: 

\textbf{Response Times}:

In three analyses, we measured the correspondence of the metrics above. In the first analysis, the similarity of the unthresholded (continuous) metrics were computed via pairwise Spearman correlations. For significance testing, we performed a permutation test with 5000 null shuffles and corrected for multiple comparisons using family-wise error rates. In the second and third analyses, the similarity of the thresholded metrics were computed via pairwise Dice coefficients. The metrics were thresholded taking the top 25\% and 10\% worst respondents for each metric. For significance testing, we again performed a permutation test with 5000 null shuffles and corrected for multiple comparisons using family-wise error rates.

\pagebreak
\section{Supplement}

\begin{table}
\centering
\setlength{\tabcolsep}{15pt}
\begin{tabular}{ rrrrr }
\toprule
& \multicolumn{2}{c}{MTurk} & \multicolumn{2}{c}{Prolific} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Total & \multicolumn{2}{c}{N=186} & \multicolumn{2}{c}{N=200} \\
\midrule
Age & N & \% & N & \% \\
\cmidrule(lr){1-5}
18-25   &      16 &   8.6 &       78 &  39.0 \\
26-35   &      76 &  40.9 &       69 &  34.5 \\
36-45   &      46 &  24.7 &       31 &  15.5 \\
46-55   &      22 &  11.8 &       13 &   6.5 \\
55+     &      26 &  14.0 &        9 &   4.5 \\
\midrule
Gender & N & \% & N & \% \\
\cmidrule(lr){1-5}
Female             &    83 &  44.6 &    112 &  56.0 \\
Male               &   103 &  55.4 &     85 &  42.5 \\
Other              &     0 &   0.0 &      3 &   1.5 \\
\midrule
Ethnicity & N & \% & N & \% \\
\cmidrule(lr){1-5}
Hispanic or Latino     &    15 &   8.1 &     10 &   5.0 \\
Not Hispanic or Latino &   168 &  90.3 &    183 &  91.5 \\
Rather not say         &     2 &   1.1 &      7 &   3.5 \\
Unknown                &     1 &   0.5 &      0 &   0.0 \\
\midrule
Race & N & \% & N & \% \\
\cmidrule(lr){1-5}
African American                 &    21 &  11.3 &      7 &   3.5 \\
Asian                                     &     5 &   2.7 &     53 &  26.5 \\
White                                     &   151 &  81.2 &    121 &  60.5 \\
Multiracial                               &     6 &   3.2 &      4 &   2.0 \\
Rather not say                            &     1 &   0.5 &     12 &   6.0 \\
\midrule
*Use other platform & N & \% & N & \% \\
\cmidrule(lr){1-5}
Yes            &      71 &  38.2 &       28 &  14.0 \\
No             &     115 &  61.8 &      172 &  86.0 \\
\bottomrule
\end{tabular}
\caption{The demographics of each sample by online labor market. On average, the samples were similar though the sample from Mechanical Turk was older (t = 6.567, p $<$ 0.001) and comprised of fewer women (z = 6.567, p = 0.011). *Note: the demographics do not include 20 participants (MTurk: N=19; Prolific: N=1) excluded for participating in the study twice, once per platform.}
\label{tab:demographics}
\end{table}

% \bibliography{main}

\end{document}