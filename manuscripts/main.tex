% set document class
\documentclass[a4paper,notitlepage,12pt]{article}

% load packages
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=0.8in]{geometry}
\usepackage{authblk}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{xr-hyper} % for cross-referencing between documents
\usepackage{hyperref}
\usepackage{float}
\usepackage{amsmath}
\usepackage[backend=biber,style=nature]{biblatex}
\usepackage[flushmargin]{footmisc}
\usepackage[flushleft]{threeparttable}
\usepackage{array}
\usepackage[font=small, labelfont=it]{caption}
\usepackage{booktabs} % for pretty tables

% specify footnote margins
\renewcommand\footnotelayout{%
  \advance\leftskip 0.7cm
  \advance\rightskip 0.7cm
 } 

% specify link style
\addbibresource{main.bib}
\renewcommand*{\bibfont}{\small}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}

% specific images path relative to the main .tex file 
\graphicspath{ {./figures/} }

% specify some custom commands
\renewcommand{\baselinestretch}{1.2}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand*{\Affilfont}{\normalsize}

% set up the author block
\setlength{\affilsep}{2em}   % set the space between author and affiliation
\author[1]{Samuel Zorowitz}
\author[1,2]{Daniel Bennett}
\author[1,3]{Yael Niv}
\affil[1]{Princeton Neuroscience Institute, Princeton University, USA}
\affil[2]{Department of Psychiatry, Monash University, Australia}
\affil[3]{Department of Psychology, Princeton University, USA}

% specify the title
\title{Inattentive responding can induce spurious associations between task behaviour and symptom measures}

% turn date off
\date{}

% Define paragraph formatting
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

% make abstract
\abstract{Abstract}

% page break before introduction
\clearpage

% Define paragraph formatting
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\section{Introduction}

In recent years, online labour markets (e.g. Amazon Mechanical Turk, Prolific, Crowdflower) have become increasingly popular as a source of research participants in the behavioral sciences \cite{stewart2017crowdsourcing}, in no small part due to the ease with which these services allow for recruitment of large, diverse samples. The advantages of online data collection have also begun to be recognized in psychiatric research \cite{chandler2016conducting}, where this method offers several distinct advantages over traditional approaches to participant recruitment. The ability to assess psychiatric symptom severity in large general-population samples makes possible large-scale transdiagnostic analysis \cite{gillan2016taking, rutledge2019machine}, and facilitates recruitment from difficult-to-reach participant populations \cite{strickland2019use}. Online labour markets also facilitate re-recruitment, making them an attractive option for validating the psychometric properties of assessment tools \cite{enkavi2019large} or studying clinical processes longitudinally \cite{kothe2019retention}.

With the advantages of online data collection also come specific drawbacks. Since participants recruited from online labour markets are typically completing experiments in their homes, they may be more likely to be distracted or multi-tasking during an experiment. Similarly, unsupervised participants may be more likely to use careless/insufficient effort (C/IE) response strategies with the intention to minimise expenditure of time and cognitive effort (e.g., responding randomly on self-report surveys or behavioural tasks). Among researchers using online labour markets, a common view is that poor-quality data resulting from C/IE responding can simply be treated as a source of unsystematic measurement error that can be overcome with increased sample sizes \cite{gillan2016taking, chandler2020participant}. Similarly, it is common practice in online behavioural research to mitigate poor-quality data using the same screening methods that are typically used in in-person data collection (e.g., excluding participants who perform at- or below-chance on behavioural tasks).

Here we wish to draw attention to an underappreciated feature of online psychiatric research using self-report symptom surveys. In such surveys, participants rate their endorsement of various psychiatric symptoms and, since most individuals in the general population tend to endorse no or few symptoms in many symptom domains, the resulting ground-truth symptom distributions tend to be heavily positively skewed. In this situation, the assumption that C/IE responding merely increases unsystematic measurement noise becomes untenable. Because of the positive skew in the ground-truth symptom distribution, participants who respond carelessly to the symptom survey are more likely to report higher levels of symptom endorsement relative to participants who complete the survey accurately \cite{chandler2020participant, ophir2020turker}. Consequently, unless C/IE survey responses are carefully identified and removed, a considerable proportion of putatively symptomatic individuals in an online sample may, in fact, be participants who have not engaged with the experiment with sufficient effort.

When participants complete both symptom surveys and behavioral tasks (a common study design in computational psychiatry), this artefact has the potential to induce spurious correlations between symptom self-reports and task behavior. Concretely, if the same participants who engage in C/IE responding on surveys (and who therefore inaccurately report high levels of psychiatric symptoms) also respond carelessly on behavioral tasks, then we would expect to observe an entirely spurious correlation emerge between greater symptom severity and worse task performance (see Figure \ref{fig:simulation}). A similar effect has been well documented in personality psychology, where the presence of careless respondents can induce correlations between questionnaires and bias estimated factors in factor analysis \cite{huang2012detecting, robinson2014inaccurate, huang2015insufficient, chandler2020participant, arias2020little}. To our knowledge, however, the potential for C/IE responding to induce spurious correlations between symptom endorsement and task behavior has not previously been noted in the burgeoning field of computational psychiatry.

\begin{figure}[t]
\includegraphics[width=16cm]{../figures/main_01.png}
\centering
\caption{Caption TK here}
\label{fig:simulation}
\end{figure}

Here we demonstrate the real risk that C/IE responding can lead to spurious symptom-task correlations in computational psychiatry research. First, we asked to what extent researchers in computational psychiatry screen self-report data from online samples. We found that the majority of these studies did not screen participants' survey data at all, and that very few followed best-practice recommendations for survey data screening. We then asked whether behavioural screening alone was sufficient to identify participants engaging in C/IE responding on psychiatric symptom surveys. In a new dataset from two separate online labour marketplaces, we found that screening based on task behaviour fails to adequately identify participants engaging in C/IE responding on surveys. Lastly, we investigated whether, under these circumstances, C/IE responding led to spurious correlations between symptom severity and task performance for positively-skewed symptom measures. Consistent with the logic set out above, we confirmed that failure to appropriately screen out C/IE survey responding in the proof-of-concept dataset that we collected would have produced a number of spurious correlations between task behaviour and self-reported symptoms of depression and anxiety.

\section{Results}

\subsection{Screening for C/IE responding is common for task behavior but not for self-report surveys}

When the rate of symptom endorsement is low in the general population, participants who engage in C/IE responding on self-report symptom measures may appear as putative symptomatic individuals. If the same participants also perform a behavioural task with insufficient effort, they may in turn induce spurious correlations between measures of task behavior and self-report symptoms. The risk for erroneous inferences, however, is attenuated if experimenters identify and remove those participants. As such, it is imperative for researchers to screen for C/IE responding in their data. The extent to which experimenters evaluate the quality of both the task and self-report data they collect is unclear. We suspect that whereas screening of task data is standard practice, the screening of self-report data is far less prevalent in online human behavioral research. 

To test the empirical support for this claim, we performed a narrative literature review of online human behavioral studies (see Methods for details of the literature search and the Github repository for the full list of included studies). Briefly, we identified studies for inclusion through searches on Google Scholar using permutations of query terms related to online labour platforms, experimental paradigms, and symptom measures. We included studies that (a) recruited participants online, (b) measured behavior on at least one experimental task, and (c) measured responses on at least one self-report symptom measure. We then evaluated whether and how each of the resulting 49 studies performed task and self-report data screening. We note that this review was not meant to be systematic, but instead to provide a representative overview of common screening methods in online behavioral studies.

Summary statistics concerning the prevalence and types of screening methods are provided in Table~\ref{tab:screening}. Of the 49 online behavioral studies meeting inclusion criteria, roughly 80\% (39/49) used at least one method to identify C/IE responding in task behavior. Of those studies, just over half relied on a single screening method, with considerable heterogeneity in behavior screening methods across studies. Most common (46\% of all studies) was identifying participants whose performance was indistinguishable from chance-level on some measure of accuracy. Almost as common (38\%) was screening based on low response variability (i.e., excluding participants who predominantly responded in the same fashion across trials, such as using only a single response key).

In contrast, only a minority (19/49, or 39\%) of online behavioral studies in our sample screened for C/IE responding in self-report symptom measures. Of those studies the most common method was the use of attention checks, which operate under the assumption that certain responses to these prompts are unlikely under attentive responding, such that participants who do not give the correct response are likely to be engaged in C/IE responding. Attention checks can be subdivided into instructed items (in which participants are explicitly told which response to select; e.g., `Please select ``Strongly Agree"'), and infrequency items (in which some responses are logically invalid; e.g., endorsing 'Agree' for the question `I competed in the 1917 Summer Olympic Games'). Of those studies that specified what type of attention check was used, instructed items were the most common method. As we discuss further below, this is notable because best-practice recommendations for data collection in personality psychology explicitly counsel against using instructed-item attention checks. Only a handful of studies employed statistical or so-called unobtrusive screening methods such as outlier detection or personal consistency. 

In sum, screening methods for C/IE responding vary widely across the studies that we reviewed. Whereas screening for C/IE responding in task behavior is relatively common for online behavioral studies, screening of self-report survey data is far less prevalent. Though at first this pattern may seem troubling, low rates of survey data screening are not necessarily an issue if screening on task behavior alone is sufficient to remove participants engaging in C/IE responding. That is, screening on survey data may be redundant if there is a high degree of correspondence between task- and survey-based screening methods. To our knowledge, however, this assumption has never been empirically tested. In the next section, we explicitly test this hypothesis in a large sample of online participants completing a battery of self-report surveys and a behavioural task.

\begin{table}[t]
\centering
\setlength{\tabcolsep}{8pt}
\begin{tabular}{lllllll}
\toprule
& \multicolumn{2}{c}{Task Screening} & \multicolumn{4}{c}{Self-Report Screening} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-7}
Frequency & \multicolumn{2}{c}{N=39 (80\%)} & \multicolumn{4}{c}{N=19 (39\%)} \\
\midrule
Measure & Accuracy & 18 (37\%) & \multicolumn{2}{l}{Attention Check} & & 17 (35\%) \\
& Variability & 15 (31\%) & & \textit{Instructed} & & 10 (20\%) \\
& Response Time & \ 7 (14\%) & & \textit{Unspecified} & & 5 (10\%) \\
& Comprehension Check & \ \ 5 (10\%) & \multicolumn{2}{l}{Unobtrusive} & & \ 4 (8\%) \\
& Other & 16 (33\%) \\
\bottomrule
\end{tabular}
\caption{The prevalence and types of task and self-report data screening practices in a sample (N=49) of online behavioral studies.}
\label{tab:screening}
\end{table}

Before continuing, we wish to briefly highlight two additional results from our literature review. First, we found that the majority of studies used only one single method to detect C/IE responding. This is worrisome from the perspective of signal detection theory. As has been previously reported \cite{desimone2018dirty}, measures of C/IE responding vary in their sensitivities: some measures will identify only the most careless participants, whereas others will flag many more participants. As such, using only a single measure runs the risk of false positives (in the case of high sensitivity) and false negatives (in the case of low sensitivity). Similarly, screening methods vary in their sensitivity to different patterns of C/IE responding \cite{curran2016methods}. Using only one measure may thus only identify a subset of careless respondents. As has been discussed elsewhere, the use of multiple screening methods is encouraged to more accurately and sensitively identify C/IE responding in a dataset \cite{curran2016methods, barends2019noncompliant}.

Second, we found that the majority of studies that did screen self-report survey data relied on instructed attention checks. This is potentially problematic, since a growing body of research disputes the efficacy of instructed items as attention checks for online studies \cite{barends2019noncompliant, thomas2017validity, hauser2016attentive}. Indeed, instructed attention checks are often discussed on forums for workers of online labour markets\footnote{Several examples of such discussions can be found at the Github repository for this project:  \url{github.com/nivlab/silver-screen}}, and scripts have even been developed with the purpose to highlight words most commonly used in instructed attention checks. Unsurprisingly, then, it has been documented that subsets of participants may actively seek out instructed items in surveys and respond randomly elsewhere \cite{barends2019noncompliant}. In line with best-practice recommendations for online survey research, therefore, we encourage experimenters to use alternate means of screening for C/IE responding for self-report survey data (e.g., infrequency-item attention checks) \cite{huang2015detecting}.

\subsection{Careless participants appear symptomatic when the overall level of symptom endorsement is low}

In the majority of the papers reviewed above, experimenters relied solely on behavioural screening to exclude participants who engaged in C/IE responding during an experiment. Nevertheless, even with behavioural screening, it is still possible that spurious behaviour-symptom correlations might emerge if there is low correspondence between behavioural and survey-based C/IE screening methods (since high correspondence between methods would entail that behavioural screening alone would exclude participants who would have failed survey screening).

In order to the measure the correspondence of screening measures estimated from task and self-report behavior, we conducted an online behavioral experiment involving a simple behavioral task and a battery of self-report psychiatric symptom measures (see Methods for details). A final sample of 386 participants from the Amazon Mechanical Turk (N=186) and Prolific (N=200) online labour markets completed a probabilistic reversal learning task and 5 self-report symptom measures. The reversal learning task required participants to learn through trial-and-error which of three choice options yielded reward most often, and was modeled after similar tasks used to probe reinforcement learning deficits in psychiatric disorders \cite{huang2017computational, waltz2007probabilistic, mukherjee_reward_2020}. The self-report measures assayed psychiatric symptoms related to depression, mania, anxiety, reward motivation, and anhedonia, and were chosen based on previous literature to have a variety of expected response distributions (symmetric and asymmetric). In line with current best-practice recommendations in personality psychology, each self-report instrument included one `infrequency' item that could be used to identify C/IE responses in survey data (see Methods for a list of infrequency items). To minimize any influence of fatigue, the entire experiment (surveys \& task) was designed to require 10 minutes on average to complete (observed mean = 10.28 minutes). For similar reasons, participants completed the surveys prior to beginning the task.

To assess the overall quality of the data, we examined the number of participants flagged by the choice accuracy and infrequency item screening measures (i.e. two most common screening measures identified in our literature review). Only 26 participants (7\%) were flagged as exhibiting choice behavior in the reversal learning task at or below chance levels. In contrast, 85 participants (22\%) responded suspiciously on one or more of the infrequency items when completing the self-report symptom measures. This discrepancy in the proportion of participants flagged by each method is consistent with previous research which have found varying levels of sensitivity to C/IE responding across screening methods \cite{desimone2018dirty}. Though we will not address platform differences here, we note the proportions of flagged participants were marginally but significantly greater on Mechanical Turk compared to Prolific for both accuracy (MTurk: N=18/186; Prolific: N=8/200; $z=2.22, p=0.026$) and infrequency (MTurk: 50/186; Prolific: 35/200; $z = 2.22, p = 0.026$) measures.

\begin{figure}[t!]
\includegraphics[width=16cm]{../figures/main_02a.png}
\centering
\caption{Caption TK here}
\label{fig:distributions}
\end{figure}

According to our hypothesis, a prerequisite for spurious behavior-symptom correlations to emerge is a mean-shift in the average level of symptom endorsement in participants engaging in C/IE responding relative to attentive participants, which might occur when the overall rate of symptom endorsement is low. As such, we visually inspected for each of the self-report measures the distribution of symptom scores in participants flagged by the infrequency measure vs. those that were not (hereafter referred to as `careless' or `attentive`, respectively; Figure \ref{fig:distributions}). In line with our predictions, the average level of symptom endorsement was noticeably exaggerated in careless participants relative to attentive participants for the symptom measures where symptom scores were positively-skewed (e.g. 7-up, 7-down, GAD-7). In contrast, where there was a greater range of symptom endorsement, the distributions of symptom scores between the two groups of participants were less noticeably distinct. 

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{6pt}
\begin{tabular}{lccccccc}
\toprule
& & \multicolumn{2}{c}{Total Score} & \multicolumn{2}{c}{Clinical Levels} & \multicolumn{2}{c}{Cronbach's $\alpha$} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
 Subscale & Skew & Attentive & Careless & Attentive & Careless & Attentive & Careless \\
\midrule
\midrule
7u    &  0.81 &    3.87 &   10.15 &     0.04 &     0.45 &    0.84 &    0.84 \\
7d    &  0.76 &    4.75 &   10.68 &     0.09 &     0.46 &    0.94 &    0.88 \\
GAD7  &  0.75 &    4.86 &    9.73 &     0.17 &     0.56 &    0.92 &    0.87 \\
BIS   &  0.78 &    7.74 &    7.92 &        - &        - &    0.83 &    0.62 \\
BAS   &  0.17 &   15.73 &   16.16 &        - &        - &    0.84 &    0.71 \\
SHAPS &  0.26 &    8.02 &   10.85 &     0.15 &     0.29 &    0.90 &    0.81 \\
PSWQ  &  0.19 &    4.78 &    6.74 &     0.07 &     0.08 &    0.93 &    0.81 \\
\bottomrule
\end{tabular}
\caption{}
\label{tab:screening}
\end{table}

To verify the infrequency item method as an accurate assay of C/IE responding, we performed three validation analyses. The first approach is to compare the proportion of participants meeting the cutoff for clinical psychopathology among careless versus attentive participants(Table \ref{tab:screening}). On the most positively-skewed measures, the fraction of participants scoring at or above clinical levels of symptom endorsement was dramatically inflated in careless responders. For example, previous studies of prevalence of generalized anxiety disorder found that roughly 5\% of the general population met the clinical threshold at a score of 10 or higher on the GAD-7 \cite{lowe2008validation, hinz2017psychometric}. Though clinical thresholds have not been established for the 7-up/7-down, recent work has suggested scores of 12 or higher as useful for discriminating between those with and without clinical levels of pathology \cite{youngstrom2020evaluating}. Though the fraction of participants meeting threshold for these measures in the attentive group was within the expected range of prevalence given epidemiological data, the exaggerated rates in the flagged group are suggestive of C/IE responding.\footnote{Interestingly, compared to previous literature the proportion of participants meeting threshold on the GAD-7 was elevated. We suspect this may reflect elevated rates of state anxiety during the COVID-19 pandemic, when this data was collected.}

The second approach was to compare the estimated internal consistency of the self-report measures between the careless and attentive groups. The logic is that, if C/IE responding manifests as a tendency to respond randomly, we should expect to see a decrease in the consistency of a measure in the careless group (as has been previously reported; \cite{maniaci2014caring, desimone2018dirty, desimone2018differential}). Accordingly, on the majority of measures we observed a reduction in Cronbach's $\alpha$ in the careless group relative to the attentive group (Table \ref{tab:screening}). A permutation test confirmed that the average decrease in internal consistency across measures was greater than would be expected by chance even controlling for the difference in the number of participants between the two groups ($t = 4.25, p = 0.006$). 

The third and final approach is to statistically quantify the degree to which participants responded to the items in the self-report symptom inventory in a stereotyped fashion; that is, to determine if participants exhibited patterns in their responses that were independent of the contents of the items. To do so, we fit a random intercept item factor analysis model \cite{maydeu2006random} to the self report data (see Methods for details). Briefly, an intercept term is estimated for each participant which quantifies their bias towards using particular response options (e.g. leftmost, rightmost) regardless of what that option signifies for a particular self-report measure. We observed a credible difference between the average value of this intercept between the two groups (stats here), such that participants suspected of C/IE responding were biased towards using the right-half of the response options on the self-report symptom inventory. This translates to a tendency to endorse greater illness on the 7-up/7-down and GAD-7 (where the rightmost options indicate greater frequency of symptoms) but translates to a tendency to endorse \emph{less} illness on the SHAPS and BIS (where the rightmost options indicate lower frequency of symptoms) despite these inventories measuring strongly correlated constructs (i.e. depression and anhedonia, anxiety and behavioral inhibition). This pattern also helps explain why the distributions of symptom scores between attentive and careless participants on the SHAPS appear to be similar despite the low rates of symptom endorsement usually observed on the SHAPS \cite{snaith1995scale}.

In sum, we found that almost a quarter of participants in our online sample were suspected of C/IE responding when screened using the infrequency-item measure, in contrast to only 7\% of participants flagged when using task accuracy as a screening measure. Importantly, participants suspected of C/IE responding were more likely to appear as symptomatic on self-report measures where symptom endorsement was rare on average. Furthermore, using three independent analyses, we identified evidence that validates the infrequency-item measures as sensitive to C/IE responding. These validation analyses also identified multiple forms of C/IE responding. Though the Cronbach's alpha result suggests at least some participants were engaging in random responding, the item response model analysis identified participants who exhibited a preference for the rightmost response options regardless of their semantic content. That C/IE responding in this data manifested in more than one way (consistent with previous reports \cite{curran2016methods}) further highlights the importance of utilizing multiple screening methods, a point we will return to in the discussion.

\subsection{Low correspondence between task and self-report measures of C/IE responding}

Next, we evaluated the degree of correspondence between behavioral and self-report screening measures. This allowed us to determine whether screening on behavior alone was sufficient to identify and remove careless participants. In line with the narrative review, we computed multiple measures of C/IE responding from each participant's task behaviour and survey responses (see Methods for full description). For task behaviour, we computed the following screening measures: response variability (the fraction of responses using only one key), accuracy (the fraction of choices of the reward-maximizing response option), win-stay lose-shift rates (the difference in likelihood of repeating the previous choice following rewarded vs. unrewarded trials), and fraction of excessively fast response times. In line with previous studies, for self-report responses we screened participants' data for C/IE responding using the following measures: infrequency items (the number of suspicious responses to infrequency items), inter-item standard deviation (ISD; the variability of responses around a participant's modal response), personal reliability (the correlation between split-halves of a participant's responses), Mahalonobis distance (the extent to which a participant is an outlier with respect to all participants), and reading time (the average time taken to respond to each item). 

To measure the degree of correspondence between these behavioral and self-report screening measures, we performed two complementary analyses. First, we computed pairwise correlations on the unthresholded (continuous) measures using Spearman's rank correlation. The resulting pairwise similarity matrices are presented in Figure \ref{fig:correspondence} (left panel). After correcting for multiple comparisons, there were few significant correlations between the behavioral and self-report measures. Only choice accuracy showed significant associations with any self-report measure (specifically the infrequency and Mahalonobis distance measures). Crucially, the size of these observed correlations were roughly half that observed for the correlations between the self-report measures. This is worrisome as it suggests that, though there is some relation between C/IE responding on tasks and self-report inventories, it is not strong enough to reliably detect careless participants in self-report data from task data alone.

\begin{figure}[!t]
\includegraphics[width=16cm, trim=0 0 3cm 0]{../figures/main_02b.png}
\centering
\caption{Caption TK here}
\label{fig:correspondence}
\end{figure}

To complement the correlational analysis, we also estimated the pairwise level of agreement on the binarized measures using the Dice similarity coefficient. This second approach quantifies the degree of overlap between the set of would-be excluded participants based on different screening measures under a common exclusion rate. Though measures like accuracy have relatively clear threshold cutoffs (e.g. chance level performance), the majority of the measures evaluated here do not. As such, we evaluated the measures with respect to the top 10\% of suspicious participants flagged by each measure, corresponding roughly to the fraction of participants having performed at chance levels on the reversal learning task. The results of the same analysis repeated for the top 25\% of suspicious participants (corresponding roughly to the fraction of participants flagged by the infrequency item measure) produced similar results, and is included in the supplement.)

The resulting pairwise similarity matrices for each approach are presented in Figure \ref{fig:correspondence} (right panel). The pattern of results are largely consistent with the correlational analysis: few pairs of task and self-report screening measures achieved levels of agreement greater than what would be expected by chance. The only significant cross-modality pair identified -- between the infrequency item and choice accuracy measures -- has a similarly coefficient less than 0.4. In other words, when these two measures are used to identify the top 10\% most suspicious participants, they agree on only two out of every five participants. Screening on choice accuracy alone (the most common method identified in our narrative review) would fail to identify the majority of participants most likely engaging in C/IE responding as determined by the infrequency items.

To conclude this section, we found across two complementary analyses that measures of C/IE responding in task and self-report data were not redundant. That is, contrary to common implicit assumptions in online behavioural research, detecting and removing careless participants based on behavioral data alone would not be sufficient for ensuring quality self-report data in our sample. Though we cannot extrapolate from this one dataset to conclude that this will always be the case, we can nevertheless conclude that experimenters cannot simply assume that screening on self-report data is redundant with screening on task behavior (and vice-versa). While tendencies towards C/IE responding in tasks and self-report inventories may be associated, our results show that both types of screening are likely to be necessary to ensure acceptable data quality.

\subsection{Spurious symptom-behaviour correlations produced by C/IE responding}

Here we detail the potential consequences of screening only on task behavior in our data. To do this, we estimated the pairwise correlations between the symptom scores of each of the self-report measures and several measures of performance on the reversal learning task. This analysis emulated a typical computational psychiatry analysis, in which the results of primary interest are the correlations between task behaviour and self-reported psychiatric symptom severity.

For each participant, we computed both descriptive and model-based measures of behaviour on the reversal learning task. Descriptive measures included accuracy (the fraction of choices of the reward-maximizing response option), points (the total number of points accumulated over the game), win-stay rates (the fraction of trials on which a participant repeated the previous trial's choice following a reward outcome), lose-shift rates (the fraction of trials on which a participant deviated from the previous trial's choice following a non-reward outcome), perseveration (the number of trials on which a participant continued to choose the previously dominant response option following a reversal). The model-based measures included the parameters of commonly-used reinforcement model of choice behavior, i.e. $Q$-learning with asymmetric learning rates (see Method for further details): choice sensitivity ($\beta$,  inverse temperature), positive learning rate ($\eta_p$, learning rate following a positive reward prediction error), negative learning rate ($\eta_n$, learning rate following a negative reward prediction error), and learning rate asymmetry ($\kappa$, difference between $\eta_p$ and $\eta_n$ in a transformed space). We chose these task performance measures as they have been previously used to assess performance in clinical samples \cite{huang2017computational, brolsma2020challenging, mukherjee_reward_2020, ritschel2017neural}. 

To understand the effects of applying different forms of screening, we estimated the correlations between each unique pairing of a self-report symptom measure and measure of behavior under four different conditions: no screening, screening only on task behavior (i.e. accuracy above chance), screening only on self-report responses (i.e. pass responses on all infrequency items), or both. The resulting pairwise behavior-symptom correlations following each screening procedure are presented in Figure \ref{fig:correlations}. To note, these correlational analyses were not corrected for multiple comparisons as the intended purpose here is to emulate a conventional analysis where fewer statistical tests would be performed, as dictated by one's hypotheses.

\begin{figure}[!t]
\includegraphics[width=16cm]{../figures/main_03a.png}
\centering
\caption{Caption TK here}
\label{fig:correlations}
\end{figure}

When no rejections based on for C/IE responding was applied (i.e. all participants in this sample were included; top left, Figure \ref{fig:correlations}), many significant correlations emerged between measures of task behavior and symptom scores. Consistent with our predictions, the majority of these correlations involved symptom measures with asymmetric total score distributions. Attending to only the most skewed measures (i.e. 7-up/7-down, GAD-7), symptom endorsement was correlated with almost every behavioral measure. That is, significant correlations were not restricted only to behavioral measures often used as proxies for participant effort (e.g. accuracy, inverse temperature $\beta$) but also to measures of previous theoretical interest. Conversely, we found few significant correlations among symptom measures with more symmetric distributions. This is despite the fact these scales measure similar symptoms and syndromes (e.g. anxiety \& worry, depression \& anhedonia). 

Next, we rejected participants based on task behavior screening (i.e. choice accuracy, removing the 7\% of participants exhibiting behavior indistinguishable from chance; top right, Figure \ref{fig:correlations}). The pattern of correlations was largely unchanged: we again found many significant correlations between measures of behavior and asymmetric symptom measures, but almost no significant correlations involving symmetric symptom measures. Looking again at the most skewed symptom measures, all correlations involving accuracy and the inverse temperature were preserved. In sum, rejections based on the most common form of behavioral screening (i.e. performance accuracy) had little effect on behavior-symptom correlations as compared to no screening.  

We then rejected participants based on self-report screening  (i.e. infrequency item, removing the 22\% of participants endorsing 1 or more suspicious responses; bottom left, Figure \ref{fig:correlations}). In stark contrast, the number of significant correlations was markedly reduced. Notably, no significant correlations remained between the 7-down depression scale and any measure of task behavior after performing rejections based on self-report behavior. Similarly, fewer (but not no) significant correlations were observed between the most skewed symptom measures and proxy measures of task attentiveness (e.g. accuracy, inverse temperature). Finally, and interestingly, the pattern of correlations were largely similar when rejections were applied based on both task and self-report screening measures. 

The preceding results suggest that many of the significant behavior-symptom correlations observed prior to rejections are in line with our predictions; that is, spurious correlations driven by C/IE responding when symptom endorsement is rare on average. This leads us to conclude that behavioural screening alone did not adequately protect against spurious symptom-behaviour correlations in the presence of skewed distributions of symptom endorsement. This can be illustrated using the example of the 7-down scale, a measure of trait depression. If we had not screened participants based on their self-report data, we would have erroneously concluded that there were many significant associations between reversal learning task performance and self-reported depression. As it was, screening on self-report data allowed us to identify that each of these depression-behaviour correlations was likely to be entirely spurious.

One possible objection to this interpretation is that the reduction in significant correlations following self-report screening and rejection was driven, not by the effective identification and removal of C/IE respondents, but simply by virtue of removing over 20\% of the sample. That is, rejections may have simply reduced the statistical power of analyses without changing the the estimated effect size. To see if this was the case, we compared the average (absolute) correlation size before and after rejections were applied (the complete tables of correlations are included in the supplement). On average, we observed a significant reduction in correlation effect sizes [add numbers] following rejections, suggesting that the reduction in significant correlations following screening was unlikely to be driven solely by a reduction in statistical power. 

Next, we investigated how spurious correlations depended on sample size. To do so, we performed a bootstrapping analysis where we held fixed the proportion of participants engaging in C/IE responding (i.e. 5\%, 10\%, 15\%, 20\%) and increased the total number of participants. Across all analyses, we measured the correlation and between the 7-down depression scale and learning rate asymmetry ($\kappa$), which we previously identified as likely exhibiting a spurious association. 

The outputs of the bootstrapping analysis are presented in Figure \ref{fig:bootstrap}. Unsurprisingly, the absolute magnitude of the behavior-symptom correlation increased with the proportion of inattentive participants (left panel). Interestingly, correlation magnitude was largely independent of sample size. Crucially, we found false-positive rates for spurious correlations \emph{increased} with increases in sample size in our data. This runs counter to a common assumption that larger sample sizes are protective against spurious correlations because they serve to mitigate measurement error. Although this is certainly the case for unsystematic measurement error, this intuition no longer holds in the regime of systematic measurement error (i.e., larger sample sizes reduce the variance of estimates, but do not alter not their bias). Instead, our results suggest that, holding the proportion of C/IE responding constant, the false-positive rate for behavioural-symptom correlations will become increasingly inflated as the sample size increases. The reason for this is that both the precision of the estimated correlation, and the statistical power to detect said correlation, increases with increasing sample size. This result runs counter to the assumption that greater measurement error in online samples can be mitigated by simply collecting more data. 

\begin{figure}[!t]
\includegraphics[width=16cm]{../figures/main_03b.png}
\centering
\caption{Caption TK here}
\label{fig:bootstrap}
\end{figure}

Of course, it is important to note that these results do not give a complete picture of the likely effects of C/IE responding on symptom-behaviour correlations. A full picture would require investigating the effects of C/IE responding over numerous different task designs, symptoms measures, sample sizes, and proportions of careless responders. Nevertheless, the results of this bootstrapping analysis suggest that spurious correlations driven by C/IE responding are likely to be a considerable source of potential error wherever large online samples are collected with asymmetric distributions of symptom endorsement. Indeed, it is also worth highlighting that false positive rates were inflated (above the expected 5\% level given $\alpha = 0.05$) even with only 5\% of the bootstrapped participant samples engaging in C/IE responding, which further highlights how pernicious this issue is.

\subsection{C/IE responding manifests as qualitatively distinct behavioral strategy}

In this final section, we employ a theory-agnostic modeling approach to better understand how participants suspected of engaging in C/IE responding performed on the probabilistic reversal learning task. This analysis was motivated by two interrelated aims. First was to better understand why there was low correspondence between screening measures derived from task and self-report data; that is, to determine why participants flagged by the infrequency items were not necessarily more likely to perform at chance. The second was to better understand how the particular pattern of behavior-symptom correlations from the previous section emerged. The results of this section are specific to the particular reversal-learning task that we used here, and it is likely that qualitatively different patterns would be likely to emerge in different behavioural tasks.

To characterize participants' choice behavior, we adapted the softmax regression model from \cite{seymour2012serotonin}. Briefly, in this model the propensity for a participant to choose an option is a function of the sum of weights corresponding to the recent history of trial events (e.g. rewarding outcomes, non-rewarding outcomes, previous choices; see Methods for details). In other words, the model estimates for each participant how much their choice on some trial was influenced by the events of the preceding five trials.

The regression weights, averaged within attentive and careless participants, for each event are presented in Figure \ref{fig:regression}. Comparing attentive participants to those suspected of engaging in C/IE responding, we observed a credible difference (i.e. 95\% highest densitivity intervals excluded zero) only for the $T-1$ weight for previous choice. Attentive participants were more likely to repeat their previous choice (i.e. greater choice hysteresis) than were careless participants. Though careless participants did appear to show less sensitivity to the history of rewards and nonrewards, the 95\% highest density intervals did not exclude zero.

\begin{figure}[!b]
\includegraphics[width=16cm]{../figures/main_03c.png}
\centering
\caption{Caption TK here}
\label{fig:regression}
\end{figure}

This result may be initially surprising, since one might expect choice hysteresis to result in more perseveration errors following contingency reversals, and since results reported in the previous section indicated that C/IE responding was associated with more perseveration errors. What is important to keep in mind, however, is that choice hysteresis is adaptive in this probabilistic reversal learning task. Since rewards in this task are probabilistic, participants should display a degree of hysteresis once the reward-maximizing response option has been identified, ignoring the occasional non-rewarding outcome from this option. In contrast to this adaptive strategy, we found that participants engaging in C/IE responding exhibited a qualitatively different behavioral strategy more akin to a win-stay lose-shift heuristic.

This, in conjunction with overall reduced outcome sensitivity ($\beta$), explains the pre-screening pattern of correlations with win-stay and lose-shift rates. But what of the learning rate asymmetries? That learning rate asymmetries ostensibly relate to C/IE responding is puzzling at first given that the softmax regression results show no credible differences in the sensitivity to history of rewarding and non-rewarding outcomes. This too can be explained by choice hysteresis. Previous work has established that, when choice hysteresis is not accounted for in reinforcement learning models, it can manifest as positive learning rate asymmetries \cite{Katahira2018-ke}. As such, given that attentive participants show increased repetition, which our reinforcement learning model did not explicitly account for, we should expect to find a negative (positive) correlation between positive (negative) learning rates and C/IE responding. Indeed, this is precisely the pattern of correlations observed above.

To conclude, the theory-agnostic analysis of task behavior revealed that careless participants were less likely to engage in adaptive choice hysteresis and instead showed a pattern of responding more akin to win-stay lose-shift heuristic. This finding helps clarify in part why we observed low correspondence between task and self-report screening measures: on average, careless participants were not engaging in random responding (which would have been detected by the accuracy screening measure) but instead in satisficing behaviors. Most importantly, the results present another hidden danger of C/IE responding. When participants utilize heuristic decision-making, this may manifest in difficult-to-predict forms when computational models are misspecified. Thus, C/IE responding may not only drive spurious correlations but also spurious inferences about what behavior-symptom correlations entail.

\section{Discussion}

Recap paragraph goes here

One way of conceptualizing the results presented here is through the lens of rational allocation of mental effort \cite{kool2018mental}. In any experiment, attentive responding is more effortful than careless responding. As such, participants completing an online task must perform a cost-benefit analysis---implicitly or otherwise---to decide how much effort to exert in responding. The variables which factor into such calculations are presumably manifold and likely include features of the experiment (e.g., task difficulty, monetary incentives), facets of the individual (e.g., subjective effort costs, intrinsic motivation, conscientiousness), and features of the online labor market itself (e.g., opportunity costs, repercussions for careless responding). To the extent to which at least some of these factors are stable within an experiment, we would expect that C/IE responding is somewhat correlated within individuals across experiment components (as we have shown here with correlations between task and self-report responding).

Viewed from the perspective of effort expenditure, our results suggest that participants appraised this cost/benefit trade-off differently for the behavioural task and the self-report surveys. Specifically, we found that only 7\% of participants were at chance-level performance in the task---compared to more than 22\% of participants who failed one or more attention check items in the self-report surveys---and that measures of C/IE responding were weakly or not at all correlated between task behavior and self-report responses. This suggests the motivation for effortful responding was greater in the behavioural tasks, though precisely why is unclear. One possibility is that we gave participants a monetary incentive for attentive responding only during the task (a common practice, according to our narrative review). A second possibility is that participants expected fewer consequences for C/IE responding during the self-report surveys, a reasonable assumption in light of how infrequently previous experiments have screened self-report data. Alternatively, participants may have found the gamified behavioural task more engaging or the self-report inventory more tedious. Whatever the reason, this discrepancy reinforces our observations concerning the inadequacy of behavioural-task screening as a stand-alone method for identifying C/IE responding. Since, in general, participants will appraise costs and benefits of effortful responding differently for behavioural tasks and self-report surveys, screening for C/IE responding on one data modality will almost never be suitable for identifying it in the other. A corollary of this is that the exact pattern of correspondence in C/IE responding across data modalities that we observed is a contingent feature of our study design; other study designs---for instance, using a more difficult behavioural task---would be expected to result in different patterns of correspondence.

One complicating factor for the argument that we have presented here is that C/IE responding may manifest in other ways than simply random responding for both behavioural tasks and self-report surveys. Indeed, in general there are more ways to respond carelessly than to respond attentively to a task or self-report inventory (e.g., random response selection, straight-lining, zig-zagging, side bias) \cite{curran2016methods}. Similarly, in the present work we observed that a win-stay lose-shift pattern emerged as a dominant strategy in the behavioral task among participants suspected of C/IE responding. As above, we posit that the specific strategy for responding that a participant adopts is likely to reflect the idiosyncratic integration of multiple perceived benefits (e.g. time saved, effort avoided) and costs (e.g. loss of performance bonuses, risk of detection and pay forfeited). As has been previously documented \cite{desimone2018dirty}, the presence of multiple response strategies makes it clear why certain screening measures are more or less likely to correlate. For example, the inter-item standard deviation and personal reliability measures are both sensitive to statistically random responding, but less sensitive to straight-lining. Most importantly, a diversity of heuristic response strategies highlights the need for many screening measures of C/IE responding, each sensitive to different heuristic strategies. 

Here we have focused on the potential for C/IE responding to result in spurious symptom-behavior correlations when rates of symptom endorsement are low, a case common to online computational psychiatry research. Beyond this, we should emphasize that a diversity of heuristic response strategies entails that there is more than one mechanism by which spurious correlations can emerge. To the extent that a only mean-shift between attentive and careless participants is a prerequisite, this is not the only situation where one might expect spurious correlations to emerge \cite{huang2015insufficient}. For example, random responding on items with \emph{high} base-rate endorsement could yield spurious correlations with precisely the opposite pattern observed here. Conversely, an abundance of straight-lining may actually suppress correlations when symptom endorsement is low. In sum, without more understanding about the various types of heuristic responding and when each is likely to occur in a sample, it is difficult to predict a priori exactly what patterns of systematic bias may arise for a given study. This is further impetus for experimenters to be wary of C/IE responding and to use a variety of screening measures to detect it. 

One potential concern with performing rigorous screening (i.e., participant exclusion) based on C/IE detection methods is that we might inadvertently introduce an overcontrol bias \cite{elwert2014endogenous}. So far we have been treating a tendency towards C/IE responding as independent from psychopathology; however, to the extent that C/IE responding reflects lack of motivation \cite{barch2015mechanisms}, avoidance of effort \cite{cohen2001impairments, culbreth2016negative}, or more frequent lapses of attention \cite{kane2016individual, robison2017neurotic}, one might hypothesise an association between these two dimensions of individual differences. In such cases, it is plausible that rigorous screening of C/IE responding might lead to the differential exclusion of truly symptomatic participants. 

Although in theory this overcontrol bias may reduce power to detect legitimate symptom-behaviour correlations, for this to seriously impact results it would have to be the case that correlations were driven by symptomatic participants whose motivation or cognition were impacted to the extent that they frequently endorsed improbable or impossible responses to infrequency-item checks (e.g., responding `Agree' to ``I competed in the 1917 Olympic Games"). We would argue that even if such participants truly are experiencing severe symptoms, there is likely to be limited utility in measuring these symptoms using a self-report measure that they are unable to complete veridically. A similar rationale underlies the widespread use of semi-structured interviews and other clinician-report measures rather than self-report measures for in-clinic psychiatric research. We would argue that, if the psychiatric phenomenon being studied is such that this issue warrants concern, the research question may be better suited to an in-person study design involving participants in the clinic who meet full diagnostic criteria than a correlational design involving an online convenience sample.

Notwithstanding the above, one response to this legitimate concern is to advocate a graded approach to screening and excluding participants \cite{Kim2018-ev}. That is, participants should be screened with respect to a multitude of measures and only the most offending participants be removed, thereby reducing the risk of inducing bias. Alternatively, another possibility is to use sensitivity analysis as an alternative to exclusion, testing whether full-sample observed correlations are robust to the exclusion of participants flagged by measures of C/IE responding. We note that the more strict screening approach used in the present study did not preclude us from identifying symptomatic participants or behavior-symptom correlations. Indeed, we found in our sample roughly 10\% of participants endorsing symptoms consistent with clinical levels of depression, and approximately 20\% consistent with clinical levels of acute anxiety. These estimates are within the realm of epidemiological norms [citations]. We also observed some positive correlations between anxiety and choice behavior that were consistent with effects found in previous literature [citations]. For example, we found higher lose-shift rates and greater learning rates following negative prediction errors. This suggests that the screening methods employed here were not so aggressive as to attenuate behavior-symptom correlations that would be expected from the literature. 

There were several notable limitations to this proof-of-concept study. We used a small set of screening measures, and did not employ other recommended procedures (e.g. logging each key/mouse interaction during survey administration to detect form filling software or other forms of speeded responding \cite{buchanan2018methods}). Thus, we cannot be confident that all of the flagged participants were indeed engaging in C/IE responding; similarly, we cannot be certain that we correctly excluded all participants engaged in C/IE responding. We studied behavior-symptom correlations for only one task and a small self-report symptom battery. It remains to be seen how generalizable our findings are, although our study design was inspired by experiments prevalent in the online computational psychiatry literature. As suggested above, future studies may find greater correspondence between task and self-report screening measures for more difficult behavioral experiments. 

The present study highlights the need for future work on the prevalence of C/IE responding in online samples and its interactions with task-symptom correlations. Many open questions remain, including under what conditions might task and symptom screening measures better correspond, what screening measures are most effective and when, and under what conditions are spurious correlations more likely to arise. One especially pressing question is how sample size affects the likelihood of obtaining spurious correlations. The results of a bootstrapping analysis in our data suggest that false positive rates are likely to increase with increasing sample sizes, but this remains to be confirmed in future empirical work. As computational psychiatry studies move towards larger samples to characterize heterogeneity in symptoms (and to increase statistical power), it will be important to understand how sample size may exaggerate the effects of systematic error. 

In summary, moving forward we would strongly recommend that experimenters employ some form of self-report screening method, preferably one recommended by the best-practices literature (see Box 1 for a list of concrete recommendations). Our narrative review found that, to date, the majority of online studies assessing behavior-symptom correlations have not used self-report screening, and our results demonstrate that stand-alone task-behaviour screening is likely to be insufficient. Our results also demonstrate that inadequate screening is likely not merely to result in increased measurement error, as commonly assumed, but may also induce spurious correlations between behavioural task metrics and self-reported psychiatric symptom levels. For these reasons, we encourage experimenters to use a variety of data-quality checks for online studies and to be transparent in their reporting of how screening was conducted, how many participants are flagged under each measure, and what thresholds are used for rejection.

More broadly, we encourage experimenters in computational psychiatry to be mindful of the myriad reasons why participants may perform worse on a behavioral task. Furthermore, we would encourage researchers wherever possible to design experiments where the signature of some symptomology is could not also be explained by C/IE responding (e.g. \cite{hunter2019excessive}). Finally, we conclude by noting that it is preferable to prevent C/IE responding than to account for it after the fact \cite{ward2018applying}. As such, we recommend researchers take pains to ensure their experiments are promote engagement, minimize fatigue and confusion, and pay fairly and ethically.

%% BOX 1 %%
\newpage
\textbf{Box 1: Recommendations for future research}

Here we offer several concrete recommendations for future research investigating symptom-behaviour correlations in online samples.

\begin{itemize}

    \item Use multiple screening methods to detect different types of C/IE responding. At a minimum, we recommend screening of both behavioral and self-report data. Within self-report data, we recommend using methods sensitive to multiple distinct patterns of C/IE responding (e.g., random responding, straight-lining, side bias) and, if possible, to log all page interactions.

    \item When collecting self-report questionnaire data, include attention-check items that flag participants who may be engaging in C/IE responding. We would recommend following best-practice guidelines [citation] in using infrequency-item checks rather than instructed-item checks. Participants flagged by suspicious responses on attention-check items should either be excluded from further analysis, or assessed using sensitivity analyses to ensure that observed full-sample correlations are robust to their exclusion.

    \item We found that spurious correlations predominantly affected self-report instruments for which the expected distributions of symptom scores were asymmetric (either positively or negatively skewed). As such, we would recommend that, \textit{ceteris paribus}, symmetrically-distributed measures of a given construct should be preferred to asymmetrically-distributed measures. Of course, given that the frequency of many psychiatric symptoms in the general population is itself positively skewed, this will not always be feasible.
    
    \item Scales with reverse-coded items may be used to quantify the consistency of participants' responses between reverse-coded and non-reverse-coded measures of the same latent construct. With some care, this may be used to identify C/IE responding even for measures that do not include attention-check items \cite{emons2009detection}. Similarly, it may be beneficial to include multiple self-report surveys of the same construct to measure consistency across scales.
    
    \item In our experience, we have found it instructive to occasionally review discussions on public forums for participants of online labour markets (e.g. Reddit, TurkerNation). Doing so helps an experimenter stay on top of what screening methods would-be participants are already aware of. Moreover, checking the forums is useful to ensure your research group is in good standing with the participant community and to better understand the labour platforms from participants' perspectives. 
    
    \item In general, it is preferable to prevent C/IE responding than to account for it after the fact \cite{ward2018applying}. As such, in designing studies, researchers should consider that confusion, fatigue, and low task motivation are likely to contribute to C/IE responding. In general, we would expect prolonged, fatiguing, confusing, or poorly-paid studies to be associated with more C/IE responding than brief, engaging, clear, and well-paid studies.
    
    \item Consider whether the online methodology is truly appropriate for the research question. In particular, if the project studies syndromes associated with considerable difficulty in task or survey engagement (e.g., severe ADHD, acute mania), then high-symptom-endorsement participants are likely to produce responses that cannot be distinguished from C/IE responding. If this is a potential concern, then correlational research with online convenience samples is likely not the appropriate methodology for the research question.

\end{itemize}

This is not an exhaustive list, and one overarching recommendation is that researchers studying individual differences in psychiatric symptom endorsement should engage meaningfully with methodological research from the psychological measurement literature, where many of these questions have long been studied [citations].

\newpage
%% END BOX 1 %%

\section{Methods}

\subsection{Literature Review}

To characterize common data screening practices in online computational psychiatry studies, we performed a narrative literature review \cite{grant2009typology}. We identified studies for inclusion through searches on Google Scholar using permutations of query terms related to online labour platforms (e.g. "mechanical turk", "prolific", "online"), experimental paradigms (e.g. "experiment", "cognitive control", "reinforcement learning"), and symptom measures (e.g. "psychiatry", "mental illness", "depression"). We included studies that (a) recruited participants online through a labour platform, (b) measured behaviour on at least one experimental task, and (c) measured responses on at least one self-report symptom measure. Through this approach, we identified for inclusion 49 studies spanning 2015 through 2020.

Two of the authors (S.Z., D.B.) then evaluated whether and how each of these studies performed data quality screening for both the collected task and self-report data. Specifically, we confirmed whether a study had performed a particular type of data screening, where the categories were themselves chosen based on commonalities in methods which emerged across the studies. In addition, we assessed the total number of screening measures each study used and how monetary bonuses were paid to participants (if at all). The fully annotated review of studies can be found in the Github repository for this project (\url{github.com/nivlab/silver-screen}). Finally, we note that this review was not meant to be systematic, but instead to provide a representative overview of common practices in online behavioral studies.

\subsection{Experiment}

\subsubsection{Sample}

409 total participants were recruited to participate in an online behavioral experiment in late June - early July, 2020. Specifically, 208 participants were recruited from Amazon Mechanical Turk (MTurk) and 201 participants were recruited from Prolific.  This study was approved by the Institutional Review Board of Princeton University (\#5291), and all participants provided written informed consent. Total study duration was approximately 10 minutes per participant. Participants received monetary compensation for their time (rate USD \$12/hr), plus an incentive-compatible bonus up to \$0.25 based on task performance. 

Participants were eligible if they resided in the United States or Canada; participants from MTurk were recruited with the aid of CloudResearch services \cite{litman2017turkprime}. (Note: This study was conducted prior to the introduction of Cloudresearch's new data quality filters \cite{cloudresearch_2020}). Following recent recommendations \cite{robinson2019tapped}, MTurk workers were not excluded based on work approval rate or number of previous jobs approved. No other exclusion criteria were applied during recruitment. 

The data from multiple participants who completed the experiment were excluded prior to analysis. Three participants (all MTurk) were excluded due to missing data. In addition, we excluded 20 participants who disclosed that they had also completed the experiment on the other platform (MTurk: N=19, Prolific: N=1). This left a final sample of N=386 participants (MTurk: N=186, Prolific: N=200) for analysis. The demographics of the sample split by labour market is provided in Table S1. Notably, the participants recruited from MTurk were older ($M = 7.7 \ \text{yrs}, t = 6.567, p < 0.001$) and comprised of fewer women ($z = 6.567, p = 0.011$). Interestingly, 38\% of MTurk users in our sample reported also completing work on Prolific, whereas only 14\% of Prolific users in our sample reported completing work on MTurk. 

\subsubsection{Experimental Task}

Participants performed a probabilistic reversal learning task, explicitly designed to be similar to previous computational psychiatry studies. On every trial of the task, participants were presented with three choice options and were required to choose one. After their choice, participants were presented with probabilistic feedback: a reward (1 point) or a non-reward (0 points). On any trial one choice option dominated the others. When chosen, the dominant option yielded reward with 80\% probability; the subordinate options yielded reward with only 20\% probability. The dominant option changed randomly to one of the two previously subordinate options every 15 trials. Participants completed 90 trials of the task (1 learning block, 5 reversal blocks). 

As a cover story, the probabilistic reversal learning task was introduced to participants as a fishing game in which each choice option was a beach scene made distinguishable by a colored surfboard with unique symbol. Participants were told they were choosing which beach to fish at. Feedback was presented as either a fish (1 point) or trash (0 points). Participants were instructed to earn the most points possible by learning (through trial-and-error) and choosing the best choice option. Participants were also instructed that the best option could change during the task, but were not informed about how often or when this would occur (see the Supplement for the complete instructions). Prior to beginning the experiment, participants had to correctly answer four comprehension questions about the instructions. Failing to correctly answer all items forced the participant to start the instructions over.

The task was programmed in jsPsych \cite{deleeuw_2015_jspsych} and distributed using custom web-application software. Both the experiment code (\url{github.com/nivlab/ThreeArmJS}) and web-software (\url{github.com/nivlab/nivturk}) are publicly available.

\subsubsection{Symptom Measures}

Prior to completing the reversal learning task, participants completed five self-report symptom measures. The symptom measures were selected for inclusion based on their frequency in clinical research, and for having an expected mixture of symmetric and asymmetric score distributions. 

\textbf{Seven-Up/Seven-Down}: The Seven-Up/Seven-Down (7u/7d; \cite{youngstrom_2013_susd}) scale is a 14-item measure of lifetime propensity towards depressive and hypomanic symptoms. It is an abbreviation of the General Behavior Inventory \cite{depue1981behavioral}, wherein only items that maximally discriminated between depression and mania were included. Items are scored on a 4-point scale from 0 ("Never or hardly ever") to 3 ("Very often or almost constantly"). Total symptom scores on both subscales range from 0 to 21, and are usually strongly right-skewed, with few participants exhibiting moderate to high levels of symptom endorsement. 

\textbf{Generalized Anxiety Disorder-7}: the Generalized Anxiety Disorder-7 (GAD-7; \cite{spitzer2006brief}) is a 7-item measure of general anxiety. The GAD-7 assesses how much a respondent has been bothered by each of seven core anxiety symptoms over the last 2 weeks. Items are scored on a 4-point scale from 0 ("not at all") to 3 ("nearly every day"). Total scores on the GAD-7 range from 0 to 21, and are usually right-skewed, with few participants exhibiting moderate to high levels of symptom endorsement.

\textbf{Behavioral Inhibition/Behavioral Activation Scales}: the Behavioral Inhibition and Behavioral Activation Scales (BIS/BAS; \cite{carver1994behavioral}) are a measure of reward and punishment sensitivity. The original 42-item measure was recently abbreviated to a 14-item measure \cite{pagliaccio2016revising}, which we use here. Items are scored on a 4-point scale from 1 ("very true for me") to 4 ("very false for me"). We note that in order to maintain consistency with the other symptom measures, the order of the BIS/BAS response options was reversed during administration such that "very false for me" and "very true for me" were the left- and rightmost anchors, respectively. 

\textbf{Snaith-Hamilton Pleasure Scale}: the Snaith-Hamilton Pleasure Scale is a 14-item measure of anhedonia \cite{snaith1995scale}. Items are scored on a 4-point scale from 0 ("strongly agree") to 3 ("strongly disagree"), where higher scores indicate greater pathology. Items are typically binarized before scoring (either disagree response receives a score of 1), and total symptom scores of 3 or greater indicate pathology. More recent research, however, has suggested using the sum of unthresholded scores instead \cite{franken2007assessment}. Here we present the unthresholded symptom scores, except for when we compute the fraction of participants meeting clinical threshold for which we use the binarized measure. 

\textbf{Penn State Worry Questionnaire}: the Penn State Worry Questionnaire is a measure of worry symptoms \cite{meyer1990development}. The original 16-item was recently abbreviated to a 3-item measure \cite{kertz2014psychometric}, which we use here. Items are scored on a 5-point scale from 1 ("not at all typical of me") to 5 ("very typical of me"), where higher scores indicate greater pathology. Total symptom scores range from 1 to 15 and are usually uniformly distributed.

\subsection{Analysis}

\subsubsection{Correspondence of screening measures}

To measure the correspondence of task- and self-report-based screening measures, we estimated a number of standard measures of data quality from each participant's task behaviour (four in total) and self-report responses (five in total). Beginning first with the self-report data, we describe each below.

\subparagraph{Infrequency items} 

Infrequency items are questions for which all or virtually all attentive participants should provide the same response. We embedded four infrequency items across the self-report measures. Specifically, we used the following questions:

\begin{enumerate}
  \item Over the last two weeks, how much time did you spend worrying about the 1977 Olympics? 
  \item Have there been times of a couple days or more when you were able to stop breathing entirely (without the aid of medical equipment)?
  \item I would feel bad if a loved one unexpectedly died.
  \item I would be able to lift a 1 lb (0.5 kg) weight.
\end{enumerate}

Prior to conducting the study, the infrequency items were piloted on an independent sample of participants to ensure that they elicited one dominant response. In the main study, we measured the number of suspicious responses made by each participant to these questions. For thresholded analyses, participants were flagged if they responded incorrectly to one or more of these items.

\subparagraph{Inter-item standard deviation} The inter-item standard deviation (ISD) is an estimate of a participant's response consistency on a self-report measure \cite{marjanovic2015inter}, defined as:

\begin{equation*}
    ISD = \sqrt{\frac{\sum^k_{i=1}(y_j - \bar{y})^2}{k-1}}
\end{equation*}

where $y_i$ is a participant's response to item $i$, $x_i$ is a participant's average score across all  items, and $k$ is the total number of items for a self-report measure. A composite ISD measure was estimated per participant by summing across each of the seven self-report scales. Larger ISD values indicate lower response consistency.

\subparagraph{Personal reliability} 

The personal reliability coefficient is an estimate of a participant's response consistency on a self-report measure, estimated by correlating the average scores from split-halves of their responses. To avoid any item-order bias, a participant's personal reliability coefficient for a particular self-report measure was computed from the average correlation from 1000 random split-halves. A composite reliability measure was generated per participant by averaging across each of the seven self-report scales. Smaller reliability coefficients indicate lower response consistency.  
\subparagraph{Mahalanobis D} The Mahalanobis distance is a multivariate outlier detection measure, which estimates how dissimilar a participant is relative to all others. For a participant $i$, the Mahalanobis D is defined as:

\begin{equation*}
    D = \sqrt{(X_i - \bar{X})^T \cdot \Sigma^{-1}_{XX} \cdot (X_i - \bar{X})^T }
\end{equation*}

where $(X_i - \bar{X})$ represents the vector of mean-centered item responses for participant $i$ and $\Sigma^{-1}_{XX}$ represents the inverted covariance matrix of all items. Greater Mahalanobis D values indicate larger deviations from the average pattern of responding.

\subparagraph{Reading Time} The reading time is the total number of seconds spent filling out a particular self-report measure, adjusted for that measure's total number of items \cite{ophir2020turker}. A total reading time estimate was estimated for each participant by summing across the adjusted time for each of the seven self-report measures. Shorter scores are indicative of less time having been spent on each item.

\subparagraph{Variability} Choice variability was defined as the fraction of trials of the most used response option per participant. Choice variability could range from 0.33 (all response options used equally) to 1.00 (only one response option used). Values closer to 1.00 are indicative of more careless responding during the task.  

\subparagraph{Accuracy} Choice accuracy was defined as the fraction of choices of the reward-maximizing response option. For a task with 90 trials and three response options, a one-tailed binomial test at $\alpha=0.05$ reveals chance-level performance to be 37 or fewer correct choices (41\%). Lower accuracy values are indicative of more inattentive responding during the task.

\subparagraph{Win-Stay Lose-Shift} Win-stay lose-shift (WSLS) measures a participant's tendency to stay with a choice option following a reward versus shifting to a new choice option following a non-reward. WSLS thus measures a participant's sensitivity to reward feedback on the screen. WSLS was estimated per participant via regression, where the current choice (stay, switch) predicted by the previous trial's outcome (reward, non-reward) and a stationary intercept. Here we used the first (slope) term to represent a participant's WSLS tendency. Lower values of this term indicate less sensitivity to reward feedback and are thus indicative of more careless responding during the task.

\subparagraph{Response Times} Suspicious response time was defined as the proportion of trials with an outlier response time, here measured as responses faster than 200ms. Greater proportions of outlier response times are indicative of more careless responding during the task.  

\subparagraph{Correspondence Analysis} We measured the correspondence of the above screening measures via two complimentary approaches. First, we computed pairwise correlations on the unthresholded (continuous) measures using Spearman's rank correlation. Second, we estimated the pairwise rate of agreement on the binarized measures using the Dice similarity coefficient (looking at the top 10\% and 25\% most suspicious respondents for each measure). The former approach estimates two measures monotonic association, whereas the latter approach estimates their agreement as to which participants were most likely engaging in C/IE responding. For significance testing, we used permutation testing wherein a null distribution of similarity scores (i.e. Spearman's correlation, Dice coefficient) was generated for each pair of screening measures by iteratively permuting participants' identities within measures and re-estimating the similarity. P-values were computed by comparing the observed score to its respective null distribution. We corrected for multiple comparisons using family-wise error rates \cite{winkler2014permutation}.

\subsubsection{Correlations between behaviour and symptom measures}

To quantify the effects of both task and self-report data screening on behavior-symptom correlations, we estimated the pairwise correlations between the symptom scores of each of the self-report measures and several measures of performance on the reversal learning task. For each participant, we computed both descriptive and model-based measures of behaviour on the reversal learning task. We describe each in turn below.

\subparagraph{Descriptive measures} Descriptive task measures included the following: accuracy, or the proportion of choices of the reward-maximizing response option; points, or the total number of points accumulated over the game; win-stay rate, or the fraction of trials on which a participant stayed with the previous choice option following a reward; lose-stay rate, or the fraction of trials on which a participant switched from the previous choice option following a non-reward; and perseveration, or the number of trials on which a participant chose the previously reward-maximizing response option following a reversal. 

\subparagraph{Model-based measures} The model-based measures were derived from a common reinforcement learning model of choice behavior, i.e. the risk-sensitive temporal difference learning model \cite{niv2012neural}. In this model, the expected value of a choice option, $Q(s)$, is learned through cycle of choice and reward feedback. Specifically, following a decision and reward feedback, the value of the chosen option is updated according to:

\begin{equation*}
    Q_{t+1}(s) = Q_{t}(s) + \eta \cdot \delta_{t}
\end{equation*}

where $\eta$ is the learning rate bounded in the range $[0,1]$ (controlling the extent to which value reflects the most recent outcomes) and $\delta$ is the reward prediction error, defined as:

\begin{equation*}
    \delta_t = r_t - Q_{t}(s)
\end{equation*}

In the risk-sensitive temporal difference learning model, there are separate learning rates for positive and negative prediction errors, such that positive and negative prediction errors have asymmetric effects on learning. For example, the effect of negative prediction errors on learned values is larger than that of positive errors if $\eta^+ < \eta^-$, and vice versa if $\eta^+ > \eta^-$.

Finally, decision-making according to the model is dictated by a softmax choice rule:

\begin{equation*}
    p(y_t = s) = \frac{\exp \left( \beta \cdot Q(s) \right)}{\sum_i^S \exp \left( \beta \cdot Q(s) \right)}
\end{equation*}

where $\beta$ is the inverse temperature, controlling a participant's sensitivity to the expected value of the choice options.

In sum then, the model-based approach describes a participant's choice behavior as a function of three parameters ($\beta, \eta^+, \eta^-$). These parameters were estimated within a Bayesian framework using Hamiltonian Monte Carlo as implemented in Stan (v2.25) \cite{stan}. Four separate chains with randomised start values each took 2000 samples from the posterior. The first 1500 samples from each chain were discarded. As a result, 2000 post-warmup samples from the joint posterior were retained. The $\hat{R}$ values for 98\% of parameters was less than 1.1, indicating acceptable convergence between chains, and there were no divergent transitions in any chain. Participant parameters were fit individually (i.e. not hierarchically) so as to prevent in parameter estimation any bias via partial-pooling between participants responding attentively versus carelessly. Parameters were sampled using non-centred parameterisations (i.e., all parameters were sampled separately from a unit normal before being transformed to the appropriate range). Of note, the learning rates were estimated via an offset method such that $\eta^+ = \eta + \kappa$ and $\eta^- = \eta - \kappa$, where $\kappa$ is an offset parameter controlling the extent of an asymmetry between the two learning rates. This parameter was also entered into the behaviour-symtom correlational analyses. Full Stan code for all models is available in the Github repository for this project (\url{github.com/nivlab/silver-screen}).

\subparagraph{Correlational analysis} Behaviour-symptom correlations (after various forms of screening and exclusion) were estimated using Spearman's rank correlation. Significance testing was performed using the percentile bootstrap method \cite{wilcox2018guide} so as to avoid making any parametric assumptions. The correlational analyses were not corrected for multiple comparisons, as the intended purpose here is to emulate a conventional analysis where fewer statistical tests would be performed, as dictated by one's hypotheses.

\subsubsection{Theory-agnostic choice analysis} In the final analysis, we use a theory-agnostic modeling approach to characterize the choice behavior of attentive participants and those suspected of C/IE responding. To do so, we adapted the softmax regression model from \cite{seymour2012serotonin}. The softmax regression model estimates the influence of the history of past rewards, non-rewards, and choices on a participant's current choice. 

Specifically, the influence of the history of particular type of event is defined as:

\begin{equation*}
    \sum_i^K w = x_{t-1} \cdot w_{t-1} + x_{t-2} \cdot w_{t-2} +  \ldots + x_{t-k} \cdot w_{t-k}
\end{equation*}

where $x_{t-i}$ is a binary indicator (0,1) denoting if an event (i.e. reward, non-reward, previous choice) occurred on trial $t-i$ and $w_{t-1}$ is the associated decision weight. These weights were estimated for rewards, non-rewards, and previous choices up to five trials in the past. 

The overall tendency to choose a particular choice option is dictated by a softmax choice rule:

\begin{equation*}
    p(y_t = i) = \frac{ \exp \left( \sum w_i^\text{reward} + \sum w_i^\text{nonreward} + \sum w_i^\text{choice} \right) }{ \sum_i \exp \left( \sum w_i^\text{reward} + \sum w_i^\text{nonreward} + \sum w_i^\text{choice} \right) }
\end{equation*}

Note that these weights were fit independently; that is, we did not employ an exponential kernel to parameterize the decay of the weights at successively distant trial lags.

In sum then, the theory-agnostic model describes a participant's choice behavior as a function of 15 parameters. These parameters were estimated within a Bayesian framework using Hamiltonian Monte Carlo as implemented in Stan (v2.25) \cite{stan}. Four separate chains with randomised start values each took 2000 samples from the posterior. The first 1500 samples from each chain were discarded. As a result, 2000 post-warmup samples from the joint posterior were retained. The $\hat{R}$ values for all parameters was less than 1.1, indicating acceptable convergence between chains, and there were no divergent transitions in any chain. Participant parameters were fit individually (i.e. not hierarchically) so as to prevent in parameter estimation any bias via partial-pooling between participants responding attentively versus carelessly. Parameters were sampled with Gaussian priors with $\mu = 0$ and $\sigma = 5$. Full Stan code for all models is available in the Github repository for this project (\url{github.com/nivlab/silver-screen}).

\printbibliography

\pagebreak
\section{Supplement}
\setcounter{table}{0}
\renewcommand{\thetable}{S\arabic{table}}

\subsection{Participant Demographics}

\begin{table}[!h]
\centering
\small
\setlength{\tabcolsep}{15pt}
\begin{tabular}{ rrrrr }
\toprule
& \multicolumn{2}{c}{MTurk} & \multicolumn{2}{c}{Prolific} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Total & \multicolumn{2}{c}{N=186} & \multicolumn{2}{c}{N=200} \\
\midrule
Age & N & \% & N & \% \\
\cmidrule(lr){1-5}
18-25   &      16 &   8.6 &       78 &  39.0 \\
26-35   &      76 &  40.9 &       69 &  34.5 \\
36-45   &      46 &  24.7 &       31 &  15.5 \\
46-55   &      22 &  11.8 &       13 &   6.5 \\
55+     &      26 &  14.0 &        9 &   4.5 \\
\midrule
Gender & N & \% & N & \% \\
\cmidrule(lr){1-5}
Female             &    83 &  44.6 &    112 &  56.0 \\
Male               &   103 &  55.4 &     85 &  42.5 \\
Other              &     0 &   0.0 &      3 &   1.5 \\
\midrule
Ethnicity & N & \% & N & \% \\
\cmidrule(lr){1-5}
Hispanic or Latino     &    15 &   8.1 &     10 &   5.0 \\
Not Hispanic or Latino &   168 &  90.3 &    183 &  91.5 \\
Rather not say         &     2 &   1.1 &      7 &   3.5 \\
Unknown                &     1 &   0.5 &      0 &   0.0 \\
\midrule
Race & N & \% & N & \% \\
\cmidrule(lr){1-5}
African American                 &    21 &  11.3 &      7 &   3.5 \\
Asian                                     &     5 &   2.7 &     53 &  26.5 \\
White                                     &   151 &  81.2 &    121 &  60.5 \\
Multiracial                               &     6 &   3.2 &      4 &   2.0 \\
Rather not say                            &     1 &   0.5 &     12 &   6.0 \\
\midrule
Use other platform & N & \% & N & \% \\
\cmidrule(lr){1-5}
Yes            &      71 &  38.2 &       28 &  14.0 \\
No             &     115 &  61.8 &      172 &  86.0 \\
\bottomrule
\end{tabular}
\caption{The demographics of each sample by online labour market. On average, the samples were similar though the sample from Mechanical Turk was older (t = 6.567, p $<$ 0.001) and comprised of fewer women (z = 6.567, p = 0.011). Note: the demographics do not include 20 participants (MTurk: N=19; Prolific: N=1) excluded for participating in the study twice, once per platform.}
\label{tab:demographics}
\end{table}

\subsection{Task Instructions}

The following are the instructions given to participants for the probabalistic reversal learning task. As a reminder, the task was given a fishing-themed cover story. Each paragraph below denotes one screen of instructions.

\textit{Welcome to the fishing game! We will now give you some instructions on how to play the game. Use the buttons below (or the arrow keys) to navigate the instructions.}

\textit{In the fishing game, there are three beaches you can fish at. Each beach has its own unique surfboard. (The colors and pictures on the surfboards are there just to help you tell the beaches apart  they dont have any special meaning other than that.)}

\textit{On each turn you will be shown three beaches, and you will choose which one you want to fish at. You can make your choice using the left, up, and right arrow keys.}

\textit{When you fish at a beach, you will either catch a fish or you will catch trash. Try to catch fish, and try not to catch trash!}

\textit{Some beaches are better than others. You are more likely to catch fish at some beaches (though you will still sometimes catch trash), and you are more likely to catch trash at other beaches (though you still sometimes catch fish).}

\textit{The beaches will change over time. As times goes by, you may be less likely to catch fish at a beach where you were previously catching many fish.}

\textit{Your goal is to catch as many fish as you can. You will receive a performance bonus up to \$0.25 that depends on how many fish you catch.}

\textit{Now we will ask you some questions about the game. You must answer all questions correctly to proceed. Feel free to read back through the instructions if there is anything you are not certain about.}

Following the instructions, participants completed a brief comprehension check where they were asked the following questions about the task:

\begin{enumerate}
    \item True or False: Your goal is to catch as many fish as you can. (True)
    \item True or False: You are more likely to catch fish at some beaches than others. (True)
    \item True or False: You will always catch fish at the best beach. (False)
    \item True or False: How likely you are to catch a fish at a beach stays the same over time. (False)
    \item True or False: The number of fish you catch will affect your final performance bonus. (True)
\end{enumerate}

Participants were required to answer all of the items correctly before they could proceed to the task. If they failed to do so, they restarted the instructions. There was no upper limit as to how many times a participant could loop through the instructions (the large majority of participants passed the comprehension check on their first try).

\subsection{Correspondence of screening measures}

\begin{table}[h!]
\centering
\begin{tabular}{rccccccccc}
\toprule
{} &      INF &      ISD &      REL &      MAH &    READ &     VAR &     ACC &   WSLS & RT \\
\midrule
INF  &        - &          &          &          &         &         &         &        &    \\
ISD  &   0.372* &        - &          &          &         &         &         &        &    \\
REL  &  -0.408* &  -0.811* &        - &          &         &         &         &        &    \\
MAH  &   0.406* &   0.843* &  -0.643* &        - &         &         &         &        &    \\
READ &   -0.111 &   0.193* &  -0.168* &    0.138 &       - &         &         &        &    \\
VAR  &   -0.061 &   -0.029 &    0.058 &   -0.024 &  -0.026 &       - &         &        &    \\
ACC  &  -0.206* &   -0.154 &    0.099 &  -0.182* &  -0.074 &   0.027 &       - &        &    \\
WSLS &    0.060 &    0.102 &   -0.089 &    0.115 &   0.103 &  -0.060 &  0.221* &      - &    \\
RT   &    0.040 &   -0.019 &    0.014 &    0.013 &  -0.158 &  -0.007 &  -0.094 &  -0.05 &  - \\
\bottomrule
\end{tabular}
\caption{Spearman correlation of measures}
\end{table}

\end{document}